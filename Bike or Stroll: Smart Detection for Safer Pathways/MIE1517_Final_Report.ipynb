{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Import Necessary Libraries to ensure the code run successfully. **Please note that we developed our project based on a dedicated server with multiple GPUs. Please adjust the system and cuda setting here accordingly to run it in your environment.**"
      ],
      "metadata": {
        "id": "K-BA3uAuNCq3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "import os,sys,re,math,datetime,time,globe,random\n",
        "# please adjust this setting accordingly for your environment\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "\n",
        "import torchvision\n",
        "# import torchvision.transforms as transforms\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.models import vision_transformer\n",
        "from torch.utils.data import DataLoader,random_split\n",
        "from torch import nn, optim\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import numpy as np\n",
        "import torchvision.models as models\n",
        "import random\n",
        "# import matplotlib as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from ultralytics import YOLO\n",
        "\n",
        "#\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import moviepy.editor as moviepy"
      ],
      "metadata": {
        "id": "S0fVx4CbMq4-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Introduction**"
      ],
      "metadata": {
        "id": "OEg7LdFSIqZD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bicycles are becoming a preference transportation mean for many young people. Based on the American Community Survey (ACS), there are roughly 780,000 people in America reporting commute by bicycle as it can reduce their carbon footprint and additionally it is efficient during rush hour. Based on the data collected by MAED IN CA, there are 16% of Canadians cycle at least once a week. The bike ownership in Canada is 36%. In Montreal, the portion of people bike to work grew from 1.3% in 1996 to 3.6% in 2016. In Toronto, the cycling commuters also grew from 1.1% to 2.7%. However, almost half of Canadians feel that it is too dangerous to cycle in their area, and on average, there are around 74 cycling fatalities per year in Canada. Every year around 41,000 cyclists die in road traffic-related crashes worldwide (report by WHO). Additionally, most cycling fatalities occurred during evening rush hour. The highest percentage, which is 16%, of cycling fatalities occurred during 16:01 and 20:00, since the environmental conditions will affect visibility, such as darkness, rain or blinding sunlight appear to have played a role in 21% of fatal cycling events.\n",
        "\n",
        "<img src=\"https://i.postimg.cc/qqKP8BYm/2024-04-05-22-52-05.png\"  width=\"350\" height=\"300\">    <img src=\"https://i.postimg.cc/vHFFG9gC/2024-04-05-22-52-25.png\"  width=\"600\" height=\"300\">\n",
        "\n",
        "Figure 1. Cyclist and Pedestrain Safety\n",
        "<br><br>\n",
        "It is with this reality in mind that the main goal of our project is to develop a lightweight cyclist detection program that can perform real-time cyclist detection in a stream-in, stream-out manner with high accuracy. Based on our research, there have been numerous studies on this topic, and object detection models nowadays have become very mature. However, they mainly focus on either predicting the existence/location of the cyclist or the direction the cyclists are moving. We believe that constantly sending alarms about the existence of a nearby biker regardless of their riding status will cause the truck driver to lose/decrease attention. For example, when a truck is stopped at a red light at an intersection, every time there is a nearby biker, those systems would send the truck driver an alarm, and typically, the bike is not moving, just waiting for the traffic light. And after constantly receiving those alarms, the driver would potentially assume that every time the cyclists are waiting for the traffic light and not riding as well, making the truck driver less cautious about it. If the biker rides the bike at this time, it could cause a tragedy. By applying our method, we not only send alarms for nearby bikers but also send different alarms for different biking statuses, then the truck driver can easily know if those bikers are in a riding status, he should pay more attention to them, thereby, decrease the risk of traffic accidents.\n",
        "\n",
        "Moreover, our project is designed (and works well) for this scenario but not limited to this scenario. According to Canadian laws, there are many places in this country where riding a bike is prohibited, like in some crosswalks and bridges, riding a bike could cause accidents. Our application can also be used in such scenarios to detect if the biker is riding the bike or just walking with it. By accurately identifying these behaviors, we can issue targeted warnings in real time, enhancing the safety of pedestrians and bicyclists. Such scenario increases the complexity of the task as the two statuses are very similar, however, our novel classification method achieved relatively great performance on this task.\n",
        "\n",
        "Regarding our system, the proposed system contains two integrated parts, a customized yolo object detection model and a classification model. The detection model takes in a frame and predicts bounding boxes of possible cyclists with values of confidence. In our project, we consider the confidence threshold as a hyperparameter and we selected it based on emprical analysis. The classification model then takes the boudingbox images and predicts if the cyclists are riding or not. Then we label the results in the corresponding frame. An overview of our system is shown in Fig.2.\n",
        "\n",
        "<img src=\"https://i.postimg.cc/tJcK6Z2n/2024-04-05-22-49-37.png\"  width=100%>\n",
        "\n",
        "Figure 2: Overview of System\n",
        "<br><br>\n",
        "\n",
        "Overall, our contributions can be summarized in the following:\n",
        "\n",
        "(1) We proposed the approach that incorporates the cyclist's stem information into the classifier model to provide extra information. And we conducted extensive experiments comparing multiple popular CNN models and showed that our approach leads to significantly increased accuracy in the classification task across all models.\n",
        "\n",
        "(2) We designed and developed a cyclist riding status detection system integrated with a detection model and a classification model that shows great performance in the complicated cyclist status classification task.\n",
        "\n",
        "(3) We proposed the approach that finds good quality bounding boxes for cyclists in the unride class by minimizing the person-bike objective function.\n",
        "\n",
        "(4) We curated around 10k high-quality cropped street view images for the cyclist riding status classification task. We have published our dataset on Kaggle, and other researchers could use our project as a starting point.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "IqPs0jAlJeGO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1.1 Dataset Description**"
      ],
      "metadata": {
        "id": "7bOca8a_EV9z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We've collected 97 videos online of cyclists riding or walking with a bike. There are 37 videos for riding class, 55 videos for un-riding class, as well as 5 mix videos containing both riding and un-riding classes. Specifically, riding videos are videos that most of its scenes are cyclists in riding status. While unride videos contains scenes that cyclists in unriding status for most of the time.\n",
        "### Data for training yolo on customized class\n",
        "We utilized the data from [Cyclist-Detection-Dataset](https://gitlab03.wpi.edu/ctang5/cyclistdetectiondataset/-/tree/main/train) to trian our yolo model on a customized class of cyclist. The dataset mainly contains 15k street views images with bounding box information of the cyclist.\n",
        "\n",
        "### Gathering cropped images for training classifer model\n",
        "We developed a piple to crop cyclist bouding box images from raw videos. Our scripts takes the raw videos and extract 3 frames per second, then each of the frames is passed to a pretrained yolo detection model for bouding box prediction. We gather around 5k cropped cyclist images in unriding status only due to it's relatively relexed bouding box determination criteria. Please refer to section 1.2.2 Object Detection for detailed explaination. To gahter high quality images of cyclists in riding status, we used a customized yolo detection model to identify the cyclists directly.\n",
        "\n",
        "After processing the raw videos with the two different detection model for each class, we obtained around 30000 riding images, but around only 5,000 unriding images. The varies length of videos constitutes the primary reason, as although we have more unriding video than riding videos, most riding video are much longer than un-riding videos and includes many similar/repeated scenes according to the content of those videos. We removed a substantial amount of images from near frames that look very similar, and mannually curated around 5000 high quality image from riding class. With all the curated iamges, here we introduce our cyclist riding status dection dataset with around 10k street view images distributed evenly in riding class and unriding class. We split this dataset into trian, test, and validation set, then we trained and tested our classifier model on this dataset. Besides, the training set, we also gathered some extra unseen cyclists videos from youtube to test our model, please refer to the result secitons for details.\n",
        "\n",
        "A draft version of our dataset is now available at: https://www.kaggle.com/datasets/optmllab01/bike-riding-cls\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2pxkyeyyLUMt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is the shape of our dataset. There are totally 10885 images and the distribution is as the graph showed:"
      ],
      "metadata": {
        "id": "IXFd-Vm5KBUE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://i.postimg.cc/NGPqSSBG/2024-04-05-22-49-21.png\"  width=\"500\" height=\"300\"> <img src=\"https://i.postimg.cc/wTy8p2jX/2024-04-05-22-49-27.png\"  width=\"400\" height=\"300\">\n",
        "\n",
        "Figure 3: Data Structure & Distribution"
      ],
      "metadata": {
        "id": "9FfXysCrKFIw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here are some data samples in our cropped dataset [Bike riding classification](https://www.kaggle.com/datasets/optmllab01/bike-riding-cls):"
      ],
      "metadata": {
        "id": "a-1XZ6i349Qc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://i.postimg.cc/1zZZMFPp/IMG-2339.jpg\"  width=\"700\" height=\"600\">\n"
      ],
      "metadata": {
        "id": "Px-CzhllKqeY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here are some frames from uncropped video:"
      ],
      "metadata": {
        "id": "0AdEcFn9Btfh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://i.postimg.cc/KvRFYFJg/IMG-2334.jpg\"  width=\"700\" height=\"500\">\n",
        "、\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "k3iJsTiZCk6O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Figure 4: Data Sample"
      ],
      "metadata": {
        "id": "KJl4ucbv6wip"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## The following fucntions are for cyclist bounding box detection approach described\n",
        "## in section 1.2.2 Object Detection.\n",
        "\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "\n",
        "# This function takes the bounding box and returns the center's coordinate\n",
        "def get_bbox_center(bbox):\n",
        "    print(bbox)\n",
        "    x_center = (bbox[0] + bbox[2]) / 2\n",
        "    y_center = (bbox[1] + bbox[3]) / 2\n",
        "    return np.array([x_center, y_center])\n",
        "\n",
        "# This method take two bouding boxes then calcualte the euc distance of them\n",
        "def get_bboxPair_dist(bboxA, bboxB, metric=\"euc\"):\n",
        "    center_bboxA, center_bboxB = get_bbox_center(bboxA), get_bbox_center(bboxB)\n",
        "    if metric==\"euc\":\n",
        "        return np.linalg.norm(center_bboxA - center_bboxB)\n",
        "\n",
        "# This method takes the two bouding boxes and calcualte their overlapping area\n",
        "def get_bboxPair_overlap_area(bboxA, bboxB):\n",
        "    xA = max(bboxA[0], bboxB[0]) # x1\n",
        "    yA = max(bboxA[1], bboxB[1]) # y1\n",
        "    xB = min(bboxA[2], bboxB[2]) # x2\n",
        "    yB = min(bboxA[3], bboxB[3]) # y2\n",
        "    interArea = max(0, xB - xA) * max(0, yB - yA)\n",
        "    return interArea\n",
        "\n",
        "\n",
        "# This function takes in the list of person bouding box and a list of bike bouding\n",
        "# boxes, then prepare the pairwise cost matrix based on objective we proposed.\n",
        "def prepare_cost_matrix(person_bbox_list, bike_bbox_list):\n",
        "    num_persons = len(person_bbox_list)\n",
        "    num_bikes = len(bike_bbox_list)\n",
        "    cost_matrix = np.zeros((num_persons, num_bikes))\n",
        "    # iteraet over all the person and bike bbox to get the cost matrix\n",
        "    for i, person in enumerate(person_bbox_list):\n",
        "        for j, bike in enumerate(bike_bbox_list):\n",
        "            dist = get_bboxPair_dist(person['xyxy'], bike['xyxy'])\n",
        "            overlap = get_bboxPair_overlap_area(person['xyxy'], bike['xyxy'])\n",
        "            # here convert the bi-obj optimization into one cost function, that is\n",
        "            # to find the combo of human and bike objs that have the largest overlapping\n",
        "            # area and the least center distance\n",
        "            cost_matrix[i, j] = dist - overlap  # min distance and max overlapping\n",
        "\n",
        "    return cost_matrix\n",
        "\n",
        "# This function takes the bounding box list of person and bike and returns the\n",
        "# best combination based on our objective function, the returned bouding boxes are\n",
        "# identifyed as bounding box of cyclist\n",
        "def match_and_merge(person_bbox_list, bike_bbox_list):\n",
        "    \"\"\"\n",
        "    Find the combo that has the min cost, then merge that combo into on obj, also\n",
        "    we increase the size of the bbox accordingly to cover the person and the bike\n",
        "    \"\"\"\n",
        "    cost_matrix = prepare_cost_matrix(person_bbox_list, bike_bbox_list)\n",
        "    row_ind, col_ind = linear_sum_assignment(cost_matrix)\n",
        "\n",
        "    merged_bbox_ind=0\n",
        "    matched_pairs = []\n",
        "    for i, j in zip(row_ind, col_ind):\n",
        "        person = person_bbox_list[i]['xyxy']\n",
        "        bike = bike_bbox_list[j]['xyxy']\n",
        "        # note that here we use the xyxy format from yolo\n",
        "        merged_box = [\n",
        "            min(person[0], bike[0]),\n",
        "            min(person[1], bike[1]),\n",
        "            max(person[2], bike[2]),\n",
        "            max(person[3], bike[3])\n",
        "        ]\n",
        "        matched_pairs.append({'bbox_id':merged_bbox_ind,\n",
        "                              'person': i,\n",
        "                              'bike': j,\n",
        "                              'merged_box': merged_box})\n",
        "        merged_bbox_ind+=1\n",
        "    return matched_pairs\n",
        "\n",
        "\"\"\"\n",
        "Function that takes in a video and then extract n frames, for each frame we apply\n",
        "our yolo model to process and then return the processed annotated images, videos,\n",
        "and cropped bboxes of potential cyclists.\n",
        "\"\"\"\n",
        "def yolo_process_video(data_home_dir, img_out_dir, vid_in_dir, model, cls, yolo_imgsz=640, out_fps=3,vid_out_dir=None):\n",
        "\n",
        "    vid_in_name,vid_in_ext = os.path.splitext(os.path.basename(vid_in_dir))\n",
        "\n",
        "    # load video and start recieving output\n",
        "    cap = cv2.VideoCapture(vid_in_dir)\n",
        "    print(f\"{cap.isOpened()}\")\n",
        "    # get specs of video\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    # print(\"here\")\n",
        "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    # extract part of the frames only to save computation and storage\n",
        "    skip_frames = int(fps / out_fps)\n",
        "    ## output to mp4\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    vid_out_dir = os.path.join(img_out_dir ,vid_in_name) # output video to same folder as frames\n",
        "    os.makedirs(vid_out_dir, exist_ok=True)\n",
        "    video_out = cv2.VideoWriter(os.path.join(vid_out_dir,f'res_{vid_in_name}{vid_in_ext}'), fourcc, out_fps, (frame_width, frame_height))\n",
        "\n",
        "    #### process each iamge with yolo\n",
        "    frame_id = 0\n",
        "    while cap.isOpened():\n",
        "        ret,frame = cap.read()\n",
        "        # print(\"cap ret frame read\")\n",
        "        if ret:\n",
        "            # skip frames\n",
        "            if frame_id % skip_frames == 0:\n",
        "                frame_rgb = frame # cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "                # save curr frame\n",
        "                # inference\n",
        "                results = model([frame_rgb], imgsz=yolo_imgsz, verbose=False)\n",
        "                #\n",
        "                # #\n",
        "                for img_index, result in enumerate(results):\n",
        "                    frame_folder_dir  = f'{vid_in_name}/res_img_{img_index}_frame_{frame_id}'\n",
        "                    frame_output_filename = f'{cls}_res_yolo_vid_{vid_in_name}_img_{img_index}_fid_{frame_id}.jpg'\n",
        "                    frame_folder_dir = os.path.join(img_out_dir, frame_folder_dir)\n",
        "                    os.makedirs(frame_folder_dir, exist_ok=True)\n",
        "\n",
        "                    # save yolo annoated result\n",
        "                    result.save(filename=f\"{frame_folder_dir}/{frame_output_filename}\")\n",
        "                    result_array = result.plot(labels=False, probs=False, boxes=False)\n",
        "                    # retireve all the bbox predicted by yolo, here, only classes labeled as human or bike are gathered\n",
        "                    # these resultes are stored in a dict which will then be further processed\n",
        "                    person_bbox_list, bike_bbox_list = [],[]\n",
        "                    bbox_cls_dict = result.names\n",
        "                    boxes_labs = result.boxes.cls.tolist()\n",
        "                    boxes_conf = result.boxes.conf.tolist()\n",
        "                    for box_id, (box, box_label, box_conf) in enumerate(zip(result.boxes.xyxy, boxes_labs,boxes_conf)):\n",
        "                        bbox_path=f\"{frame_folder_dir}/box_id_{box_id}_{bbox_cls_dict[box_label]}.jpg\"\n",
        "                        x1,y1,x2,y2 = [int(_) for _ in box.tolist()]\n",
        "                        x1, y1 = max(x1, 0), max(y1, 0)\n",
        "                        x2, y2 = min(x2, frame_width), min(y2, frame_height)\n",
        "                        # print(x1,y1,x2,y2)\n",
        "                        # cropped_img = frame_rgb[y1:y2, x1:x2]\n",
        "                        # cv2.imwrite(bbox_path, cropped_img)\n",
        "                        # store the current bounding box\n",
        "                        if box_label==0:\n",
        "                            person_bbox_list.append({\"bbox_ind\":box_id, \"conf\":box_conf, \"xyxy\":[x1,y1,x2,y2]})\n",
        "                        if box_label==1:\n",
        "                            bike_bbox_list.append({\"bbox_ind\":box_id, \"conf\":box_conf, \"xyxy\":[x1,y1,x2,y2]})\n",
        "\n",
        "\n",
        "                    # for each image label out the merged person and bike bounding box\n",
        "                    person_bike_merged_bbox_list = match_and_merge(person_bbox_list, bike_bbox_list)\n",
        "                    for pb_bbox_ind, pb_bbox in enumerate(person_bike_merged_bbox_list):\n",
        "                        pb_bbox_path=f\"{frame_folder_dir}/{cls}_res_cropbp_vid_{vid_in_name}_img_{img_index}_fid_{frame_id}_bbox_{pb_bbox_ind}.jpg\"\n",
        "                        x1,y1,x2,y2 = pb_bbox['merged_box']\n",
        "                        label=f\"{cls}_bike_person\"\n",
        "                        cv2.rectangle(result_array, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "                        # Put the label near the rectangle\n",
        "                        cv2.putText(result_array, label, (x1, max(y1 - 10, 0)), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
        "                        # save boxes to local\n",
        "                        pb_bbox_img = result_array[y1:y2, x1:x2]\n",
        "                        cv2.imwrite(pb_bbox_path, pb_bbox_img)\n",
        "\n",
        "\n",
        "                    video_out.write(result_array) # f'{cls}_res_img_{img_index}_fid_{frame_id}.jpg'\n",
        "                    cv2.imwrite(f\"{frame_folder_dir}/{cls}_res_yolobp_vid_{vid_in_name}_img_{img_index}_fid_{frame_id}.jpg\", result_array)\n",
        "\n",
        "            frame_id += 1\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    # print(\"here end\")\n",
        "    cap.release()\n",
        "    video_out.release()"
      ],
      "metadata": {
        "id": "uOpGBohGCQXR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This part of code is to generate data of cropped bouding boxes of human and bike\n",
        "# to finetune the classifier. In our dataset, we have scence with ride, unride and\n",
        "# mixed. Ride videos are videos that most of its scences are cyclists in riding status.\n",
        "# While unride videos are videos that most of its scences are cyclists in unriding status.\n",
        "# We use this method to gather cropped iamges mainly for unride class.\n",
        "# Please refer to section 1.2.2 Object Detection for more details.\n",
        "\n",
        "model = YOLO('./model/yolov8m.pt')\n",
        "#\n",
        "cls_list=['ride', 'unride', 'mixed']\n",
        "# cls=cls_list[0]\n",
        "for cls in cls_list:\n",
        "    task_name = \"bike_data\"\n",
        "    data_home_dir=\"/home/yuliang/1517_proj/ultralytics/datasets\"\n",
        "    img_out_dir=f\"/home/yuliang/1517_proj/ultralytics/datasets/processed_data/{task_name}_img/{cls}\"\n",
        "    vid_out_dir=f\"/home/yuliang/1517_proj/ultralytics/datasets/processed_data/{task_name}_vid/{cls}\"\n",
        "    os.makedirs(img_out_dir, exist_ok=True)\n",
        "    os.makedirs(vid_out_dir, exist_ok=True)\n",
        "\n",
        "    raw_data_dir = f\"/home/yuliang/1517_proj/ultralytics/datasets/raw_data/{task_name}_vid/{cls}/\"\n",
        "    for file in os.listdir(raw_data_dir):\n",
        "        if file.endswith('.mp4'):\n",
        "            vid_in_dir = os.path.join(raw_data_dir, file)\n",
        "            yolo_process_video(data_home_dir, img_out_dir, vid_in_dir, model=model, cls=cls, yolo_imgsz=640, out_fps=3)"
      ],
      "metadata": {
        "id": "WwHM5yVor6WV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We train yolo model with customized cyclist class with bouding box CDD datasets.\n",
        "\n",
        "# import os\n",
        "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
        "\n",
        "customized_yolo_model =YOLO('/home/yuliang/1517_proj/ultralytics/checkpoint/yolov8s.pt')\n",
        "results = customized_yolo_model.train(data='/home/yuliang/1517_proj/ultralytics/datasets/cyclist.yaml',\n",
        "                      epochs=200,\n",
        "                      imgsz=640,\n",
        "                      patience=30,\n",
        "                      batch=64,\n",
        "                      lr0=0.01,\n",
        "                      lrf=0.01, # final lr = (lr0 * lrf)\n",
        "                      weight_decay=5e-4,\n",
        "                      dropout=0.1,\n",
        "                      pretrained=True,\n",
        "                      seed=0,\n",
        "                      box=8.5,\n",
        "                      device=\"cuda:1\",\n",
        "                      name=\"detect_s\",\n",
        "                      project=\"/home/yuliang/1517_proj/ultralytics/checkpoint/yolov8n\")"
      ],
      "metadata": {
        "id": "IJksBTxZPHV4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1.2 Model structure**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GaWA3KacVzuo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1.2.1 Overview**\n",
        "\n",
        "\n",
        "This section details the architecture and workflow of our system designed for detecting cyclists and classifying their biking status within images. Our system integrates two main components: an object detection model using YOLOv8 detection model [2], and a subsequent classification model that determines if detected individuals are actively riding their bicycles. We process the input video by extracting 'n' frames per second; each frame is then passed to the detection model to identify potential cyclists through a bounding box prediction task. Then for each bouding box identified within a frame, we pass it to our classifier model, which predicts whether the observed cyclist is in riding or unriding status, defining a binary classifcation task."
      ],
      "metadata": {
        "id": "5sPr0A8412j3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1.2.2 Object Detection**\n",
        "\n",
        "We adapted the implementation of YOLOv8n detection model from UltraLytics, a lightweight deep learning model (3.2m parameters) optimized for real-time object detection. The performacne and the inference time of this model aligns with our ultimate goal to facilitate a stream-in, stream-out scenario for real-time classification tasks.\n",
        "\n",
        "Initially, we utilized the vanilla pretrained yolov8 model which dosen't have the ability to detect the class of cyclist. To address this issuse, for each image, we search for all the bouding boxes of person and bikes. Then we optimize for person-bike combinations that minimize their center distance and maximize their overlapping area as in formula (1) where D is a function that calcuate center distance of two object, and A is a function that calcuate overlapping area of two objects.\n",
        "\n",
        "$$\n",
        "\\begin{equation}\n",
        "\\underset{\\substack{i \\in |P|, j \\in |B| \\\\ |b| = |B|}}{\\arg\\min} \\left( D(p_i, b_j) - A(p_i, b_j) \\right) \\tag{1}\n",
        "\\end{equation}\n",
        "$$\n",
        "\n",
        "\n",
        "An illustrate of this process is shown in Fig.5.\n",
        "<br><br>\n",
        "<img src=\"https://i.postimg.cc/MKcXjm3v/bbox-opt.png\" width=75%>\n",
        "\n",
        "Fig.5 : process of finding person-bike combinations\n",
        "\n",
        "Upon experimentation, we observed that the bounding box quality detected by this method was relatively poor. Specifically, the method could misidentify combinations of a standing bike and a nearby pedestrian as a cyclist, often linking a bike to a random nearby person instead of the actual cyclist. We investigated into this issue, and we found that since our video resources are captured with a single camera, we don't have depth information of them. So people who stand closer to the camera appears larger in the images, and they tend to have more overlapping with the bike than the actual cyclists, which leads to such individuals incorrectly been identified as cyclists. For instance, Fig 6 (a) and Fig.6 (b) shows that when the person in the red shirt stands closer to the camera, she has the same center distance to the bike but larger overlapping area with the bike than the actual biker, causing the incorrect identification.\n",
        "<br><br>\n",
        "<img src=\"https://i.postimg.cc/T15cPgrW/demo2-opt-gif.gif\" width=55%>\n",
        "\n",
        "Fig.6 (a): gif illustration of misclassification caused by the lack of depth information\n",
        "<br><br>\n",
        "<img src=\"https://i.postimg.cc/Njy8FYYr/frames.png\" width=65%>\n",
        "\n",
        "Fig.6 (b): image illustration of misclassification caused by the lack of depth information\n",
        "<br><br>\n",
        "Given this limitation, we determined that while this approach was not viable for sourcing high-quality images of riding cyclists due to the increased manual selection workload. However, it proved effective for generating quality images of non-riding scenarios, such as pedestrians near bikes, people standing by bikes, or holding them. Therefore, we retained this method to generate cropped iamges of cyclists, from which we manuaaly select images for unriding classes.\n",
        "\n",
        "To gather high-quality cropped iamges for ridding classes, we opted to customize a YOLOv8 detection model for our specific cyclist class. We utilized the CyclistDetectionDataset, which primarily features street view images with cyclist bounding boxes. This dataset contains around 13k training, 1k testing, and 500 validation iamges, providing suficient data source for our goal. By training the pretrained vanila YOLOv8n model on this dataset, we endowed it the ability to predict our custom class -cyclist. During the training process, we used a combination of loss with respect to bouding box prediction and classficaiton, with more weights on bouding box predicting. We selected our best model with bouding box loss of 1.396 and classification loss of 0.64411, producing generally accurate results according to our experiments.\n",
        "<br><br>\n",
        "<img src=\"https://i.postimg.cc/02sYCdVN/system-r1.png\" width=\"60%\" >\n",
        "\n",
        "Fig. 7: left part of system"
      ],
      "metadata": {
        "id": "L9VIWLTkINf3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1.2.3 Classifier model**\n",
        "\n",
        "The classification of cycling status based on images alone introduces a significant challenge due to the visual similarities between riding and non-riding postures, both characterized by a person's proximity and significant overlap with a bicycle. Considering factors including image resolution, background complexity, this similarity often makes it difficult, even for humans, to distinguish between the two states. Current popular models have been proven their capacity on more complicated task, however, they didn't show satisfactory result according to our experiment.\n",
        "\n",
        "Our investigated into hundreds of cyclist videos revealed that there seems to be a pattern of the body stem information with the biking status.\n",
        "In light of this discovery, and as a novelty in our project, we incorporated person stem inforamtion to the classifer model as we believe this extra information could help the model better understand what is happening in the image and the model can potentially make use of the location of arms and legs.\n",
        "\n",
        "<img src=\"https://i.postimg.cc/257C0V7q/demo3-stem-gif.gif\" width=\"50%\" >\n",
        "\n",
        "Fig.8 (a): Stem information of a biker\n",
        "\n",
        "Our classifier contains 3 integrated parts as shown in Fig 8. Initially, a CNN model learns the image representations, and a PoseNet model identifies body key points of the cyclist, which are then further analyzed by a MLP to derive a meaningful representation. Subsequently, the CNN output is flattened and concatenated with the keypoints representation. Then this combined vector is fed into our MLP classifier to perform the binary classification.\n",
        "\n",
        "<img src=\"https://i.postimg.cc/GpqGLWR8/model1.png\" width=\"70%\" >\n",
        "\n",
        "Fig.8 (b): Structure of classifer model with 3 integrated parts\n"
      ],
      "metadata": {
        "id": "mtabXPRNIc0i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Model Training and Comparison**"
      ],
      "metadata": {
        "id": "l4OyvV3YCLGQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2.1 Preprocessing and Load Image Dataset**\n",
        "At this stage, we are gathering cropped images from a previous part of our project, segmenting them into training, validation, and test sets. We utilized ImageFolder to load the data, and each image was transformed to a size of 224x224 pixels. Given that the images were cropped using bounding boxes, some are rectangular rather than square. To minimize image distortion and enhance the likelihood that the YOLO model could accurately extract stem information, we resized the images to maintain their aspect ratio, ensuring one dimension was 224 pixels. We then padded the images with zeros to achieve a uniform size of 224x224 pixels.\n",
        "\n",
        "It's important to note that although the YOLO pose model is trained on 640x640 data, some images in our dataset do not possess such high resolution. Furthermore, due to limited computational resources, our hardware is unable to process 640x640 size images with complex neural network models. Therefore, we tested our model with 224x224 size images."
      ],
      "metadata": {
        "id": "YWnfIry_ib9-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image, ImageOps\n",
        "\n",
        "class ResizeAndPad:\n",
        "    def __init__(self, desired_size):\n",
        "        self.desired_size = desired_size\n",
        "\n",
        "    def __call__(self, img):\n",
        "        # Resize the image to maintain aspect ratio with one side being 224\n",
        "        aspect_ratio = img.width / img.height\n",
        "        if img.width < img.height:  # Width is the smaller dimension\n",
        "            new_width = self.desired_size\n",
        "            new_height = int(self.desired_size / aspect_ratio)\n",
        "        else:  # Height is the smaller dimension or they are equal\n",
        "            new_height = self.desired_size\n",
        "            new_width = int(self.desired_size * aspect_ratio)\n",
        "\n",
        "        # Use Image.Resampling.LANCZOS for high-quality downsampling\n",
        "        img = img.resize((new_width, new_height), Image.Resampling.LANCZOS)\n",
        "\n",
        "        # Calculate padding\n",
        "        delta_width = self.desired_size - new_width\n",
        "        delta_height = self.desired_size - new_height\n",
        "        padding = (delta_width // 2, delta_height // 2, delta_width - (delta_width // 2), delta_height - (delta_height // 2))\n",
        "\n",
        "        # Pad the resized image\n",
        "        img = ImageOps.expand(img, padding, fill=0)  # Fill is the color for padding, 0 means black\n",
        "\n",
        "        return img\n"
      ],
      "metadata": {
        "id": "CSxfswrZidO0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Define a transformation that resizes images to 224x224 pixels and normalizes them. Then, load the training, validation, and test datasets with these settings, ensuring all images are consistently formatted for model training and evaluation."
      ],
      "metadata": {
        "id": "zOiDncV__ARM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Now integrate this into your transforms.Compose\n",
        "transform = transforms.Compose([\n",
        "    ResizeAndPad(224),  # Custom transform to resize and pad\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "base_dir = 'F:/UT_MIE/MIE1517/Project/finallllll/finallllll'\n",
        "\n",
        "# Load each dataset separately\n",
        "train_dataset = datasets.ImageFolder(root=f'{base_dir}/train', transform=transform)\n",
        "val_dataset = datasets.ImageFolder(root=f'{base_dir}/val', transform=transform)\n",
        "test_dataset = datasets.ImageFolder(root=f'{base_dir}/test', transform=transform)\n",
        "\n",
        "# Display the shape of each dataset\n",
        "print(f\"train: {len(train_dataset)}, val: {len(val_dataset)}, test: {len(test_dataset)}\")"
      ],
      "metadata": {
        "id": "VS6tvcYcih6g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The show_random_images_with_labels function displays a selection of transformed images alongside their labels, demonstrating that the images have been correctly transformed and labeled, as illustrated in the picture below. This visualization aids in verifying the accuracy of the image preprocessing and labeling process. The images are loaded properly!"
      ],
      "metadata": {
        "id": "LwpWUnCwBBjG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://i.postimg.cc/m21b7JvX/show-random-transformed-images.png\"  width=\"1000\" height=\"180\">\n",
        "\n",
        "Figure 9: Transformed Images\n"
      ],
      "metadata": {
        "id": "kSmPRDn2BXVT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def show_random_images_with_labels(dataset, num_images=5):\n",
        "    # Randomly select `num_images` samples from the dataset\n",
        "    selected_indices = random.sample(range(len(dataset)), num_images)\n",
        "    selected_samples = [dataset[i] for i in selected_indices]\n",
        "\n",
        "    # Create a figure for plotting\n",
        "    fig, axes = plt.subplots(1, num_images, figsize=(15, 3))\n",
        "    for i, (image, label) in enumerate(selected_samples):\n",
        "        # Convert image to numpy array for plotting\n",
        "        image_np = image.numpy().transpose((1, 2, 0))\n",
        "        mean = np.array([0.485, 0.456, 0.406])\n",
        "        std = np.array([0.229, 0.224, 0.225])\n",
        "        image_np = std * image_np + mean  # Unnormalize\n",
        "        image_np = np.clip(image_np, 0, 1)  # Clip values to be between 0 and 1\n",
        "\n",
        "        # Plot\n",
        "        if num_images == 1:\n",
        "            ax = axes\n",
        "        else:\n",
        "            ax = axes[i]\n",
        "        ax.imshow(image_np)\n",
        "        ax.axis('off')  # Turn off axis numbers and ticks\n",
        "        if label == 1:\n",
        "            ax.set_title(f\"Label: Unride({label})\")  # Set the title to the image's label\n",
        "        else:\n",
        "            ax.set_title(f\"Label: Ride({label})\")  # Set the title to the image's label\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Assuming your `train_dataset` is already loaded and transformed\n",
        "show_random_images_with_labels(train_dataset)"
      ],
      "metadata": {
        "id": "JPz-tJY5BA7u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The count_labels_by_folder function is designed to display the distribution of the dataset by showing how many images in the training, validation, and test datasets belong to the ride and unride classes. This function is crucial for ensuring that the dataset used for training is balanced, allowing for a fair representation of both classes and potentially leading to more accurate and generalized model performance."
      ],
      "metadata": {
        "id": "CZpwXd9F_CXX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def count_labels_by_folder(datasets):\n",
        "    # Initialize counters for each dataset\n",
        "    counts = {\n",
        "        'Train': {'ride': 0, 'unride': 0},\n",
        "        'Validation': {'ride': 0, 'unride': 0},\n",
        "        'Test': {'ride': 0, 'unride': 0}\n",
        "    }\n",
        "\n",
        "    for name, dataset in datasets.items():\n",
        "        # Iterate through the images and labels in the ImageFolder\n",
        "        for path, _ in dataset.imgs:\n",
        "            if 'unride' in path:\n",
        "                counts[name]['unride'] += 1\n",
        "            elif 'ride' in path:\n",
        "                counts[name]['ride'] += 1\n",
        "\n",
        "    return counts\n",
        "\n",
        "# Assuming train_dataset, val_dataset, and test_dataset are ImageFolder instances\n",
        "datasets = {\n",
        "    'Train': train_dataset,\n",
        "    'Validation': val_dataset,\n",
        "    'Test': test_dataset\n",
        "}\n",
        "\n",
        "# Count and print the labels\n",
        "label_counts = count_labels_by_folder(datasets)\n",
        "for dataset_name, counts in label_counts.items():\n",
        "    print(f\"{dataset_name}: Ride: {counts['ride']}, Unride: {counts['unride']}\")"
      ],
      "metadata": {
        "id": "a-U3zt4WimLW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "In this section, we compare the performance of various deep learning models in predicting ride and unride classes. Additionally, we'll explore whether incorporating stem or pose information into the models enhances their predictive accuracy."
      ],
      "metadata": {
        "id": "E7Byei59jSce"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2.2 Models Without Stem**"
      ],
      "metadata": {
        "id": "SQZRlMublZNZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In our project, since we are conducting binary classification, we have chosen to use accuracy and the F1 score as performance measures, and then plotted the confusion matrix to view detailed classification results.\n",
        "\n",
        "Initially, we experimented with three models with out stem information: CNN model, SqueezeNet model and ResNet18 model.\n"
      ],
      "metadata": {
        "id": "WO4fHltwhf0U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def set_seed(seed_value=42, use_cuda=True):\n",
        "    random.seed(seed_value)\n",
        "    np.random.seed(seed_value)\n",
        "    torch.manual_seed(seed_value)\n",
        "    if use_cuda and torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed_value)\n",
        "        torch.cuda.manual_seed_all(seed_value)"
      ],
      "metadata": {
        "id": "MVHz-bOwikef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "To enhance our understanding of the training process and better interpret the validation results, we utilized the 'show_misclassified_images function'. This function helps in displaying images that were incorrectly classified, allowing us to visually assess where and why our model may be making mistakes."
      ],
      "metadata": {
        "id": "A-VjcgfPcNLt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def show_misclassified_images(misclassified_samples, num_images=30):\n",
        "    # Calculate the number of rows needed to display the images\n",
        "    num_rows = num_images // 5 + (num_images % 5 > 0)\n",
        "    fig, axes = plt.subplots(num_rows, 5, figsize=(20, num_rows * 4))  # Adjust the size as needed\n",
        "\n",
        "    # Flatten the axes array for easy indexing\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    for i, (image, label, pred) in enumerate(misclassified_samples[:num_images]):\n",
        "        image_np = image.numpy().transpose((1, 2, 0))\n",
        "        mean = np.array([0.485, 0.456, 0.406])\n",
        "        std = np.array([0.229, 0.224, 0.225])\n",
        "        image_np = std * image_np + mean  # Unnormalize\n",
        "        image_np = np.clip(image_np, 0, 1)  # Clip values to be between 0 and 1\n",
        "\n",
        "        ax = axes[i]\n",
        "        ax.imshow(image_np)\n",
        "        ax.set_title(f\"Label: {int(label)}\\nPred: {int(pred.squeeze().item())}\", fontsize=10)\n",
        "        ax.axis('off')\n",
        "\n",
        "    # Hide any unused subplots if the number of images is not a multiple of 5\n",
        "    for j in range(i + 1, num_rows * 5):\n",
        "        axes[j].axis('off')\n",
        "\n",
        "    plt.subplots_adjust(wspace=0.4, hspace=0.6)  # Adjust spacing between images\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "cIxrgQLTlm_L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For training our model, we used 'BCEWithLogitsLoss' for its binary classification of ride/unride states, alongside the Adam optimizer known for its efficiency. Our training function outputs key metrics: training/validation accuracy, performance curves, a confusion matrix, and shows misclassified validation images for a clear understanding of model performance.\n",
        "\n",
        "Accuracy is the most intuitive performance measure; it represents the ratio of correctly predicted observations to the total number of observations. However, accuracy alone can be misleading if the data is imbalanced, as it does not distinguish between the types of errors our model makes. To gain deeper insight, we utilize the F1 Score, which is the harmonic mean of precision and recall. This score is particularly useful in scenarios where class distribution is uneven. Finally, we visualize our results with a confusion matrix, enabling us to observe instances of true positives, true negatives, false positives, and false negatives."
      ],
      "metadata": {
        "id": "gFEP1NHicq8R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function that train the model\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n",
        "\n",
        "def train_model(train_dataset, val_dataset, model, num_epochs, bsz, lr, patience,\n",
        "                weight_decay=0, use_gpu=True, plot_=False, confusion = True, misclass = True):\n",
        "    set_seed()\n",
        "    #\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() and use_gpu else \"cpu\")\n",
        "    print(f\"curr device: {device}\")\n",
        "\n",
        "    # prepare data loader\n",
        "    train_loader = DataLoader(train_dataset, batch_size=bsz, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=bsz, shuffle=False)\n",
        "    # data parallel\n",
        "    # model.to(device)\n",
        "    # loss function and optimizer\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    #\n",
        "    # early stop\n",
        "    best_val_loss = float('inf')\n",
        "    epochs_no_improve=0\n",
        "\n",
        "    # stats\n",
        "    train_corr_list = np.full(num_epochs, np.nan)\n",
        "    train_loss_list = np.full(num_epochs, np.nan)\n",
        "    val_corr_list = np.full(num_epochs, np.nan)\n",
        "    val_loss_list = np.full(num_epochs, np.nan)\n",
        "    train_f1_list = np.full(num_epochs, np.nan)\n",
        "    val_f1_list = np.full(num_epochs, np.nan)\n",
        "    train_precision_list = np.full(num_epochs, np.nan)\n",
        "    train_recall_list = np.full(num_epochs, np.nan)\n",
        "    val_precision_list = np.full(num_epochs, np.nan)\n",
        "    val_recall_list = np.full(num_epochs, np.nan)\n",
        "\n",
        "    best_acc_epoch = 0  # Track the epoch with the highest validation accuracy\n",
        "    best_val_accuracy = 0\n",
        "\n",
        "    # train loop\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_loss, train_correct = 0.0,0\n",
        "        train_bsz, val_bsz = 0,0\n",
        "        train_preds, train_labels_list = [], []\n",
        "        with tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch+1}/{num_epochs}\") as t:\n",
        "            for batch_ind, (inputs, labels) in t:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                optimizer.zero_grad()\n",
        "                # forward\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels.unsqueeze(1).float())\n",
        "                # backward\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                # stats\n",
        "                train_loss += loss.item()\n",
        "                # for binary cls case\\\n",
        "                preds = torch.round(torch.sigmoid(outputs))  # Using sigmoid and rounding for binary classification\n",
        "                train_correct += (preds.squeeze() == labels).sum().item()\n",
        "                train_bsz += labels.size(0)\n",
        "\n",
        "                # Collecting labels and predictions for metrics calculation\n",
        "                train_labels_list.extend(labels.cpu().numpy())\n",
        "                train_preds.extend(preds.squeeze().detach().cpu().numpy())\n",
        "\n",
        "        avg_train_acc = train_correct / train_bsz\n",
        "        avg_train_loss = train_loss / len(train_loader)\n",
        "        train_loss_list[epoch] = avg_train_loss\n",
        "        train_corr_list[epoch] = avg_train_acc\n",
        "        train_f1 = f1_score(train_labels_list, train_preds)\n",
        "        train_f1_list[epoch] = train_f1\n",
        "        train_precision = precision_score(train_labels_list, train_preds)\n",
        "        train_precision_list[epoch] = train_precision\n",
        "        train_recall = recall_score(train_labels_list, train_preds)\n",
        "        train_recall_list[epoch] = train_recall\n",
        "\n",
        "\n",
        "        # val per epoch\n",
        "        val_labs, val_preds = [],[]\n",
        "        #\n",
        "        val_loss, val_correct = 0.0,0\n",
        "        misclassified_samples = []\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels.unsqueeze(1).float())\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                # stats and confusion matrix\n",
        "                # binary cls case\n",
        "                preds = torch.round(torch.sigmoid(outputs))\n",
        "\n",
        "                misclassified = (preds.squeeze(1) != labels)\n",
        "                for i, mis in enumerate(misclassified):\n",
        "                    if mis:\n",
        "                        misclassified_samples.append((inputs[i].cpu(), labels[i].cpu(), preds[i].cpu()))\n",
        "\n",
        "                # print(preds.shape, preds.squeeze(1))\n",
        "                val_correct += (preds.squeeze(1) == labels).sum().item()\n",
        "                val_bsz += labels.size(0)\n",
        "                # prepare pres and labs for cf matrix\n",
        "                val_labs.extend(labels.cpu().numpy())\n",
        "                val_preds.extend(preds.squeeze(1).cpu().numpy())\n",
        "\n",
        "\n",
        "        avg_val_acc = val_correct / val_bsz\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "        val_loss_list[epoch]=avg_val_loss\n",
        "        val_corr_list[epoch] = avg_val_acc\n",
        "        val_f1 = f1_score(val_labs, val_preds)\n",
        "        val_f1_list[epoch] = val_f1\n",
        "        val_precision = precision_score(val_labs, val_preds)\n",
        "        val_precision_list[epoch] = val_precision\n",
        "        val_recall = recall_score(val_labs, val_preds)\n",
        "        val_recall_list[epoch] = val_recall\n",
        "\n",
        "        ## log info\n",
        "        print(f'epoch {epoch+1}/{num_epochs} | train | loss: {avg_train_loss:.6f}, acc: {avg_train_acc:.4f}, Precision: {train_precision:.4f}, Recall: {train_recall:.4f}, f1: {train_f1:.4f}')\n",
        "        print(f'epoch {epoch+1}/{num_epochs} | val | loss: {avg_val_loss:.6f} acc: {avg_val_acc:.4f}, Precision: {val_precision:.4f}, Recall: {val_recall:.4f}, f1: {val_f1:.4f}')\n",
        "\n",
        "\n",
        "        if avg_val_acc > best_val_accuracy:\n",
        "            best_val_accuracy = avg_val_acc\n",
        "            best_acc_epoch = epoch\n",
        "            # Store labels and predictions for the best epoch\n",
        "            best_val_labels_list = val_labs.copy()\n",
        "            best_val_preds_list = val_preds.copy()\n",
        "\n",
        "        ## early stop based on val dataset result\n",
        "        # check if val loss improved\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            epochs_no_improve = 0\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "            print(f\"no improvement in val loss for {epochs_no_improve} epochs.\")\n",
        "            if epochs_no_improve == patience:\n",
        "                    print(f\"patient exceeded, early stop triggered at epoch {epoch}\")\n",
        "                    break\n",
        "    print(f\"\\nBest val corr observed at {np.nanargmax(val_corr_list)+1}: {np.nanmax(val_corr_list)}\")\n",
        "    print(f\"Best val loss observed at {np.nanargmin(val_loss_list)+1}: {np.nanmin(val_loss_list)}\")\n",
        "    print(f\"Best val f1 observed at {np.nanargmax(val_f1_list)+1}: {np.nanmax(val_f1_list)}\")\n",
        "\n",
        "    if confusion == True:\n",
        "        cm = confusion_matrix(best_val_labels_list, best_val_preds_list)\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=[0, 1], yticklabels=[0, 1])\n",
        "        plt.title(f'Confusion Matrix for Best Val Acc Epoch: {best_acc_epoch + 1}')\n",
        "        plt.ylabel('True Label')\n",
        "        plt.xlabel('Predicted Label')\n",
        "        plt.show()\n",
        "\n",
        "    if plot_ == True:\n",
        "        # Generating the plot\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.plot(train_loss_list, label='train Loss')\n",
        "        plt.plot(val_loss_list, label='val Loss')\n",
        "        # Adding title and labels\n",
        "        plt.title(f'loss for bsz:{bsz} epc:{epoch+1} lr:{lr}')\n",
        "        plt.xlabel('Epochs')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.legend()\n",
        "        # Display the plot\n",
        "        plt.show()\n",
        "        # Generating the plot\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.plot(train_corr_list, label='train accr')\n",
        "        plt.plot(val_corr_list, label='val accr')\n",
        "        # Adding title and labels\n",
        "        plt.title(f'accr for bsz:{bsz} epc:{epoch+1} lr:{lr}')\n",
        "        plt.xlabel('Epochs')\n",
        "        plt.ylabel('Accr')\n",
        "        plt.legend()\n",
        "        # Display the plot\n",
        "        plt.show()\n",
        "\n",
        "    if misclass == True:\n",
        "        show_misclassified_images(misclassified_samples)"
      ],
      "metadata": {
        "id": "V0dGVp1Zli11"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2.2.1 Simple 3-layers CNN**"
      ],
      "metadata": {
        "id": "IWCNK6oDmG8L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The simple CNN model comprises 3 convolutional layers and 2 linear layers, combined with pooling and ReLU activation functions."
      ],
      "metadata": {
        "id": "8HTPpTpEesFO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)  # Input channels = 3 (RGB)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)  # Pooling layer\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
        "        self.fc1 = nn.Linear(128 * 28 * 28, 512)  # Adjust the input size accordingly\n",
        "        self.fc2 = nn.Linear(512, 1)  # Output layer for binary classification\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))  # 224x224x3 -> 112x112x32\n",
        "        x = self.pool(F.relu(self.conv2(x)))  # 112x112x32 -> 56x56x64\n",
        "        x = self.pool(F.relu(self.conv3(x)))  # 56x56x64 -> 28x28x128\n",
        "\n",
        "        x = x.view(-1, 128 * 28 * 28)  # Flatten the output for the dense layer\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)  # Sigmoid activation for binary classification\n",
        "        return x"
      ],
      "metadata": {
        "id": "RqGj-PE_lkUq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The CNN model without STEM information is trained as following:"
      ],
      "metadata": {
        "id": "DzI2odpbeubz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load a Simple CNN model\n",
        "model = SimpleCNN()\n",
        "\n",
        "# data parallel\n",
        "# model = nn.DataParallel(model, device_ids=[0, 1])\n",
        "model.to(torch.device(\"cuda\"))\n",
        "\n",
        "train_model(train_dataset=train_dataset,\n",
        "            val_dataset=val_dataset,\n",
        "            model=model,\n",
        "            num_epochs=10,\n",
        "            bsz=64,\n",
        "            lr=0.001,\n",
        "            patience=3,\n",
        "            use_gpu=True,\n",
        "            plot_=True)"
      ],
      "metadata": {
        "id": "TDtczW8LmL5T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2.2.2 SqueezeNet**"
      ],
      "metadata": {
        "id": "vQpVmC1ynbF4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "SqueezeNet, a pre-trained model designed to achieve AlexNet-level accuracy with significantly fewer parameters and a smaller model size, is ideal for deployment in environments with limited computational resources or for applications requiring real-time processing. And the SqueezeNet model with out stem information is trained as following:"
      ],
      "metadata": {
        "id": "mWi0Xhg6f0Sg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple 2 layer MLP for classification\n",
        "class mlp1(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(mlp1, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 512)\n",
        "        self.fc2 = nn.Linear(512, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Load a pre-trained SqueezeNet model\n",
        "model = models.squeezenet1_0(weights=True)\n",
        "model.classifier = nn.Sequential(\n",
        "    nn.AdaptiveAvgPool2d(1),\n",
        "    nn.Flatten(),\n",
        "    mlp1(512, 1)\n",
        ")\n",
        "\n",
        "# data parallel\n",
        "# model = nn.DataParallel(model, device_ids=[0, 1])\n",
        "model.to(torch.device(\"cuda:0\"))\n",
        "\n",
        "train_model(train_dataset=train_dataset,\n",
        "            val_dataset=val_dataset,\n",
        "            model=model,\n",
        "            num_epochs=10,\n",
        "            bsz=64,\n",
        "            lr=0.001,\n",
        "            patience=3,\n",
        "            use_gpu=True)"
      ],
      "metadata": {
        "id": "SrbI3xUZndzT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2.2.3 ResNet18**"
      ],
      "metadata": {
        "id": "inTj6S74nRfs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ResNet18 is an 18-layer pre-trained model, known for its use of residual connections that help to overcome the vanishing gradient problem. Which is ideal for our project. And the ResNet18 model with out stem information is trained as following:"
      ],
      "metadata": {
        "id": "akzhnkqAgDmy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load a pre-trained ResNet18 model\n",
        "model = models.resnet18(weights=True)\n",
        "# Remove the fully connected layer\n",
        "num_ftrs = model.fc.in_features\n",
        "# Replace with MLP\n",
        "model.fc = mlp1(num_ftrs, output_dim=1)\n",
        "\n",
        "# data parallel\n",
        "# model = nn.DataParallel(model, device_ids=[0, 1])\n",
        "model.to(torch.device(\"cuda:0\"))\n",
        "\n",
        "train_model(train_dataset=train_dataset,\n",
        "            val_dataset=val_dataset,\n",
        "            model=model,\n",
        "            num_epochs=10,\n",
        "            bsz=64,\n",
        "            lr=0.001,\n",
        "            patience=3,\n",
        "            use_gpu=True)"
      ],
      "metadata": {
        "id": "kxHU9l4YnYG5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2.3 Models With PoseNet**\n",
        "\n"
      ],
      "metadata": {
        "id": "wW40WGxuoymu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The main adjustment on the training function for models with pose information is:\n",
        "* Find the posture keypoints information of every batch\n",
        "* feed both img information and keypoint information to the model to make prediction"
      ],
      "metadata": {
        "id": "3GMo0Il7jPBU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function that train the model\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n",
        "import copy\n",
        "\n",
        "def train_PoseNetCNN(train_dataset, val_dataset, model, num_epochs, bsz, lr, patience,\n",
        "                     weight_decay=0, use_gpu=True, plot_=False, PoseNetModel=None,\n",
        "                     confusion=True, misclass=True, model_dir=None):\n",
        "    set_seed()\n",
        "\n",
        "    if not os.path.exists(model_dir):\n",
        "        os.makedirs(model_dir, exist_ok=True)\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() and use_gpu else \"cpu\")\n",
        "    print(f\"curr device: {device}\")\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=bsz, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=bsz, shuffle=False)\n",
        "\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    epochs_no_improve = 0\n",
        "\n",
        "    train_corr_list = np.full(num_epochs, np.nan)\n",
        "    train_loss_list = np.full(num_epochs, np.nan)\n",
        "    val_corr_list = np.full(num_epochs, np.nan)\n",
        "    val_loss_list = np.full(num_epochs, np.nan)\n",
        "\n",
        "    # New metrics storage\n",
        "    train_precision_list = np.full(num_epochs, np.nan)\n",
        "    train_recall_list = np.full(num_epochs, np.nan)\n",
        "    train_f1_list = np.full(num_epochs, np.nan)\n",
        "    val_precision_list = np.full(num_epochs, np.nan)\n",
        "    val_recall_list = np.full(num_epochs, np.nan)\n",
        "    val_f1_list = np.full(num_epochs, np.nan)\n",
        "\n",
        "    best_acc_epoch = 0  # Track the epoch with the highest validation accuracy\n",
        "    best_val_accuracy = 0\n",
        "\n",
        "    best_model_wts=None\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_loss, train_correct = 0.0, 0\n",
        "        train_total, val_total = 0, 0\n",
        "        train_preds, train_labels_list, val_preds, val_labels_list = [], [], [], []\n",
        "\n",
        "        # Training loop\n",
        "        with tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch+1}/{num_epochs}\") as t:\n",
        "            for batch_ind, (inputs, labels) in t:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                with torch.no_grad():\n",
        "                    # Assuming PoseNetModel and necessary preprocessing are correctly defined\n",
        "                    res = PoseNetModel(inputs, verbose=False)\n",
        "                    res_keypoints = torch.stack([i.keypoints[0].xy if i.keypoints[0].conf!=None else torch.zeros(1,17,2).to(device) for i in res ]).squeeze(1)\n",
        "                    mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1).to(device)\n",
        "                    std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1).to(device)\n",
        "                    inputs = (inputs - mean) / std\n",
        "\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(inputs, res_keypoints)\n",
        "                loss = criterion(outputs, labels.unsqueeze(1).float())\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                train_loss += loss.item()\n",
        "                preds = torch.round(torch.sigmoid(outputs))\n",
        "                train_correct += (preds.squeeze() == labels).sum().item()\n",
        "                train_total += labels.size(0)\n",
        "\n",
        "                # Collecting labels and predictions for metrics calculation\n",
        "                train_labels_list.extend(labels.cpu().numpy())\n",
        "                train_preds.extend(preds.squeeze().detach().cpu().numpy())\n",
        "\n",
        "        # Validation loop\n",
        "        model.eval()\n",
        "        val_loss, val_correct = 0.0, 0\n",
        "        misclassified_samples = []\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "                res = PoseNetModel(inputs, verbose=False)\n",
        "                res_keypoints = torch.stack([i.keypoints[0].xy if i.keypoints[0].conf!=None else torch.zeros(1,17,2).to(device) for i in res ]).squeeze(1)\n",
        "\n",
        "                ################\n",
        "                mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1).to(device)\n",
        "                std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1).to(device)\n",
        "                inputs = (inputs - mean) / std\n",
        "\n",
        "                outputs = model(inputs, res_keypoints)\n",
        "                loss = criterion(outputs, labels.unsqueeze(1).float())\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                preds = torch.round(torch.sigmoid(outputs))\n",
        "\n",
        "                misclassified = (preds.squeeze(1) != labels)\n",
        "                for i, mis in enumerate(misclassified):\n",
        "                    if mis:\n",
        "                        misclassified_samples.append((inputs[i].cpu(), labels[i].cpu(), preds[i].cpu()))\n",
        "\n",
        "                val_correct += (preds.squeeze(1) == labels).sum().item()\n",
        "                val_total += labels.size(0)\n",
        "\n",
        "                # Collecting labels and predictions for metrics calculation\n",
        "                val_labels_list.extend(labels.cpu().numpy())\n",
        "                val_preds.extend(preds.squeeze(1).detach().cpu().numpy())\n",
        "\n",
        "        # Calculating metrics\n",
        "        avg_train_acc = train_correct / train_total\n",
        "        avg_val_acc = val_correct / val_total\n",
        "        train_precision = precision_score(train_labels_list, train_preds)\n",
        "        train_recall = recall_score(train_labels_list, train_preds)\n",
        "        train_f1 = f1_score(train_labels_list, train_preds)\n",
        "        val_precision = precision_score(val_labels_list, val_preds)\n",
        "        val_recall = recall_score(val_labels_list, val_preds)\n",
        "        val_f1 = f1_score(val_labels_list, val_preds)\n",
        "\n",
        "        # Updating lists with metrics\n",
        "        train_corr_list[epoch] = avg_train_acc\n",
        "        val_corr_list[epoch] = avg_val_acc\n",
        "        avg_train_loss = train_loss / len(train_loader)\n",
        "        train_loss_list[epoch] = avg_train_loss\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "        val_loss_list[epoch] = avg_val_loss\n",
        "        train_precision_list[epoch] = train_precision\n",
        "        train_recall_list[epoch] = train_recall\n",
        "        train_f1_list[epoch] = train_f1\n",
        "        val_precision_list[epoch] = val_precision\n",
        "        val_recall_list[epoch] = val_recall\n",
        "        val_f1_list[epoch] = val_f1\n",
        "\n",
        "        print(f'Epoch {epoch+1}: Train Loss: {train_loss / len(train_loader):.4f}, Acc: {avg_train_acc:.4f}, Precision: {train_precision:.4f}, Recall: {train_recall:.4f}, F1: {train_f1:.4f}')\n",
        "        print(f'         Val Loss: {val_loss / len(val_loader):.4f}, Acc: {avg_val_acc:.4f}, Precision: {val_precision:.4f}, Recall: {val_recall:.4f}, F1: {val_f1:.4f}')\n",
        "\n",
        "\n",
        "        if avg_val_acc > best_val_accuracy:\n",
        "            best_val_accuracy = avg_val_acc\n",
        "            best_acc_epoch = epoch\n",
        "            # Store labels and predictions for the best epoch\n",
        "            best_val_labels_list = val_labels_list.copy()\n",
        "            best_val_preds_list = val_preds.copy()\n",
        "\n",
        "         # check if val loss improved\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            epochs_no_improve = 0\n",
        "\n",
        "            best_model_wts = copy.deepcopy(model.state_dict())\n",
        "            torch.save(best_model_wts, f\"{model_dir}/{model.name}_best.pth\")\n",
        "\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "            print(f\"no improvement in val loss for {epochs_no_improve} epochs.\")\n",
        "            if epochs_no_improve == patience:\n",
        "                    print(f\"patient exceeded, early stop triggered at epoch {epoch}\")\n",
        "                    break\n",
        "    print(f\"\\nBest val corr observed at {np.nanargmax(val_corr_list)+1}: {np.nanmax(val_corr_list)}\")\n",
        "    print(f\"Best val loss observed at {np.nanargmin(val_loss_list)+1}: {np.nanmin(val_loss_list)}\")\n",
        "    print(f\"Best val F1 observed at {np.nanargmax(val_f1_list)+1}: {np.nanmax(val_f1_list)}\")\n",
        "\n",
        "    # Plotting if requested\n",
        "    if plot_ == True:\n",
        "        # Generating the plot\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.plot(train_loss_list, label='train Loss')\n",
        "        plt.plot(val_loss_list, label='val Loss')\n",
        "        # Adding title and labels\n",
        "        plt.title(f'loss for {model.name} bsz:{bsz} epc:{epoch+1} lr:{lr}')\n",
        "        plt.xlabel('Epochs')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.legend()\n",
        "        # Display the plot\n",
        "        plt.show()\n",
        "        # Generating the plot\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.plot(train_corr_list, label='train accr')\n",
        "        plt.plot(val_corr_list, label='val accr')\n",
        "        # Adding title and labels\n",
        "        plt.title(f'accr for {model.name} bsz:{bsz} epc:{epoch+1} lr:{lr}')\n",
        "        plt.xlabel('Epochs')\n",
        "        plt.ylabel('Accr')\n",
        "        plt.legend()\n",
        "        # Display the plot\n",
        "        plt.show()\n",
        "\n",
        "    # Plotting confusion matrix for the best accuracy epoch\n",
        "    if confusion:\n",
        "        # Recalculate or retrieve predictions for the best_acc_epoch if necessary\n",
        "        cm = confusion_matrix(best_val_labels_list, best_val_preds_list)\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=[0, 1], yticklabels=[0, 1])\n",
        "        plt.title(f'Confusion Matrix for Best Val Acc Epoch: {best_acc_epoch + 1}')\n",
        "        plt.ylabel('True Label')\n",
        "        plt.xlabel('Predicted Label')\n",
        "        plt.show()\n",
        "\n",
        "    if misclass == True:\n",
        "        show_misclassified_images(misclassified_samples)"
      ],
      "metadata": {
        "id": "2-qPYV6_oxq9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2.3.1 PoseNet + simple CNN**"
      ],
      "metadata": {
        "id": "7ToZAhA0qg5S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The PoseNetCNN model combines traditional CNN features with pose key points data. This model processes key points through a separate multi-layer perceptron (MLP), merges them with CNN-derived features, and further processes the combined data through an extensive MLP for classification. The model employs Leaky ReLU activations for the key points MLP to mitigate the dying ReLU problem."
      ],
      "metadata": {
        "id": "MatJ5Ol5ksVy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PoseNetCNN(nn.Module):\n",
        "    def __init__(self, num_keypoints=17, num_classes=1, lrelu_tr=0.1):\n",
        "        super(PoseNetCNN, self).__init__()\n",
        "        self.name=\"PoseNetCNN\"\n",
        "\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),  # 112x112x32\n",
        "            nn.Conv2d(32, 64, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),  # 56x56x64\n",
        "            nn.Conv2d(64, 128, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),  # 28x28x128\n",
        "        )\n",
        "\n",
        "\n",
        "        self.yoloMLP = nn.Sequential(\n",
        "            nn.Linear(num_keypoints * 2, 128),  # 17 * 2 (x, y)\n",
        "            nn.LeakyReLU(lrelu_tr),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.LeakyReLU(lrelu_tr)\n",
        "        )\n",
        "\n",
        "        self.combinedMLP = nn.Sequential(\n",
        "            nn.Linear(128 * 28 * 28 + 64, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1024, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, keypoints):\n",
        "        # CNN\n",
        "        x = self.cnn(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "\n",
        "        # yoloMLP\n",
        "        keypoints = keypoints.view(keypoints.size(0), -1) #\n",
        "        yoloMLP_out = self.yoloMLP(keypoints)\n",
        "\n",
        "        # concat\n",
        "        concat_features = torch.cat((x, yoloMLP_out), dim=1)\n",
        "        #\n",
        "        output = self.combinedMLP(concat_features)\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "ISSoKt1BnY9c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "yoloPoseModel = YOLO('F:/UT_MIE/MIE1517/Project/yolov8n-pose.pt')\n",
        "\n",
        "PoseNetCNN_model = PoseNetCNN()\n",
        "# # data parallel\n",
        "# # model = nn.DataParallel(model, device_ids=[0, 1])\n",
        "PoseNetCNN_model.to(torch.device(\"cuda:0\"))\n",
        "\n",
        "train_PoseNetCNN(train_dataset=train_dataset,\n",
        "            val_dataset=val_dataset,\n",
        "            model=PoseNetCNN_model,\n",
        "            num_epochs=10,\n",
        "            bsz=64,\n",
        "            lr=0.001,\n",
        "            patience=3,\n",
        "            PoseNetModel=yoloPoseModel,\n",
        "            use_gpu=True,\n",
        "            plot_=True,\n",
        "            model_dir='models')"
      ],
      "metadata": {
        "id": "dD3QV-pRm6li"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2.3.2 PoseNet + SqueezeNet**"
      ],
      "metadata": {
        "id": "m9W-7D-Xmr7f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The PoseNetSqueeze class modifies the SqueezeNet architecture to specialize in pose estimation by integrating pose key points with SqueezeNet's extracted features. Key adjustments include removing SqueezeNet's original classifier and replacing it with a new sequence tailored for pose estimation, incorporating a separate MLP for processing key points data, and combining these features through an additional MLP for final predictions."
      ],
      "metadata": {
        "id": "XrKHgBW81keS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PoseNetSqueeze(nn.Module):\n",
        "    def __init__(self, num_keypoints=17, num_classes=1, lrelu_tr=0.1, pretrained=True):\n",
        "        super(PoseNetSqueeze, self).__init__()\n",
        "        self.name = \"PoseNetSqueeze\"\n",
        "\n",
        "        # Load a pretrained SqueezeNet model\n",
        "        self.squeezenet = models.squeezenet1_0(pretrained=pretrained)\n",
        "\n",
        "        # Remove the classifier of SqueezeNet\n",
        "        # SqueezeNet uses a classifier that starts with a 'dropout' layer\n",
        "        self.squeezenet.features = nn.Sequential(*list(self.squeezenet.children())[0])\n",
        "\n",
        "        # SqueezeNet's final convolution layer outputs 512 channels with a 13x13 spatial dimension when the input is 224x224\n",
        "        # We need to adapt the combined MLP to this new structure\n",
        "        final_conv = nn.Conv2d(512, num_classes, kernel_size=1)\n",
        "        self.squeezenet.classifier = nn.Sequential(\n",
        "            nn.Dropout(p=0.5),\n",
        "            final_conv,\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.AdaptiveAvgPool2d((1, 1))\n",
        "        )\n",
        "\n",
        "        # MLP for processing keypoints\n",
        "        self.yoloMLP = nn.Sequential(\n",
        "            nn.Linear(num_keypoints * 2, 128),  # 17 * 2 (x, y)\n",
        "            nn.LeakyReLU(lrelu_tr),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.LeakyReLU(lrelu_tr)\n",
        "        )\n",
        "\n",
        "        # Combined MLP\n",
        "        # Adjust the input features of the combinedMLP to match SqueezeNet's output\n",
        "        self.combinedMLP = nn.Sequential(\n",
        "            nn.Linear(num_classes + 64, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1024, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, keypoints):\n",
        "        # Process images through the SqueezeNet model\n",
        "        x = self.squeezenet.features(x)\n",
        "        x = self.squeezenet.classifier(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "\n",
        "        # yoloMLP\n",
        "        keypoints = keypoints.view(keypoints.size(0), -1)\n",
        "        yoloMLP_out = self.yoloMLP(keypoints)\n",
        "\n",
        "        # Concatenate SqueezeNet features and keypoints MLP output\n",
        "        concat_features = torch.cat((x, yoloMLP_out), dim=1)\n",
        "\n",
        "        # Combined MLP\n",
        "        output = self.combinedMLP(concat_features)\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "68kbRvSZmrdD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "yoloPoseModel = YOLO('F:/UT_MIE/MIE1517/Project/yolov8n-pose.pt')\n",
        "\n",
        "PoseNetCNN_model = PoseNetSqueeze()\n",
        "# # data parallel\n",
        "# # model = nn.DataParallel(model, device_ids=[0, 1])\n",
        "PoseNetCNN_model.to(torch.device(\"cuda:0\"))\n",
        "\n",
        "train_PoseNetCNN(train_dataset=train_dataset,\n",
        "            val_dataset=val_dataset,\n",
        "            model=PoseNetCNN_model,\n",
        "            num_epochs=10,\n",
        "            bsz=64,\n",
        "            lr=0.001,\n",
        "            patience=3,\n",
        "            PoseNetModel=yoloPoseModel,\n",
        "            use_gpu=True,\n",
        "            plot_=True,\n",
        "            model_dir='models')"
      ],
      "metadata": {
        "id": "vKbDboC9nDsR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2.3.3 PoseNet + ResNet18**"
      ],
      "metadata": {
        "id": "yDQKTAb2mgGr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The PoseNetResnet class adapts the ResNet18 architecture for pose estimation tasks by incorporating additional pose key points information alongside the deep features extracted by ResNet18. This model leverages a pretrained ResNet18, omitting its original fully connected layers to use it purely as a feature extractor. An MLP is introduced specifically for processing the pose key points data, which is then combined with the ResNet18 features. The combined data is further processed through a custom MLP for the final prediction."
      ],
      "metadata": {
        "id": "_BI5VC-B_riX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PoseNetResnet(nn.Module):\n",
        "    def __init__(self, num_keypoints=17, num_classes=1, lrelu_tr=0.1, pretrained=True):\n",
        "        super(PoseNetResnet, self).__init__()\n",
        "        self.name = \"PoseNetResnet\"\n",
        "\n",
        "        # Load a pretrained ResNet18 model\n",
        "        self.resnet = models.resnet18(pretrained=pretrained)\n",
        "\n",
        "        # Remove the fully connected layers (fc) of ResNet18\n",
        "        self.resnet = nn.Sequential(*(list(self.resnet.children())[:-2]))\n",
        "\n",
        "        # MLP for processing keypoints\n",
        "        self.yoloMLP = nn.Sequential(\n",
        "            nn.Linear(num_keypoints * 2, 128),  # 17 * 2 (x, y)\n",
        "            nn.LeakyReLU(lrelu_tr),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.LeakyReLU(lrelu_tr)\n",
        "        )\n",
        "\n",
        "        # Combined MLP\n",
        "        # ResNet18 with default settings ends with 512 channels at 7x7 spatial dimensions, when the input is 224x224\n",
        "        self.combinedMLP = nn.Sequential(\n",
        "            nn.Linear(512 * 7 * 7 + 64, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1024, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, keypoints):\n",
        "        # Process images through the ResNet18 model\n",
        "        x = self.resnet(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "\n",
        "        # yoloMLP\n",
        "        keypoints = keypoints.view(keypoints.size(0), -1)\n",
        "        yoloMLP_out = self.yoloMLP(keypoints)\n",
        "\n",
        "        # concat\n",
        "        concat_features = torch.cat((x, yoloMLP_out), dim=1)\n",
        "\n",
        "        # combined MLP\n",
        "        output = self.combinedMLP(concat_features)\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "bhJsBoRvme9c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "yoloPoseModel = YOLO('F:/UT_MIE/MIE1517/Project/yolov8n-pose.pt')\n",
        "\n",
        "PoseNetCNN_model = PoseNetResnet()\n",
        "# # data parallel\n",
        "# # model = nn.DataParallel(model, device_ids=[0, 1])\n",
        "PoseNetCNN_model.to(torch.device(\"cuda:0\"))\n",
        "\n",
        "train_PoseNetCNN(train_dataset=train_dataset,\n",
        "            val_dataset=val_dataset,\n",
        "            model=PoseNetCNN_model,\n",
        "            num_epochs=10,\n",
        "            bsz=64,\n",
        "            lr=0.001,\n",
        "            patience=3,\n",
        "            PoseNetModel=yoloPoseModel,\n",
        "            use_gpu=True,\n",
        "            plot_=True,\n",
        "            model_dir='models')"
      ],
      "metadata": {
        "id": "NU5lpq24nr4z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  **2.3.4 Hyperparameter Tuning**"
      ],
      "metadata": {
        "id": "GnoHRPu_aKer"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also adjusted the hyperparameters of our models, particularly for PoseNet with a simple CNN and PoseNet with ResNet. We experimented with various batch sizes, learning rates, and then incorporated a dropout layer into the model, fine-tuning the dropout rate. We discovered that the optimal model performance was achieved with a batch size of 64, a learning rate of 0.001, and a dropout rate of 0.2. However, the training and validation accuracies for models with different hyperparameter sets did not show significant variation."
      ],
      "metadata": {
        "id": "aTHfAjx2aV61"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PoseNetCNN_drop(nn.Module):\n",
        "    def __init__(self, num_keypoints=17, num_classes=1, lrelu_tr=0.1, dropout_cnn=0.2, dropout_mlp=0.2):\n",
        "        super(PoseNetCNN_drop, self).__init__()\n",
        "        self.name = \"PoseNetCNN_drop\"\n",
        "\n",
        "        # Convolutional layers with dropout after activation and pooling\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Dropout(dropout_cnn),\n",
        "            nn.Conv2d(32, 64, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Dropout(dropout_cnn),\n",
        "            nn.Conv2d(64, 128, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Dropout(dropout_cnn))\n",
        "\n",
        "        # MLP for YOLO keypoints with dropout after LeakyReLU\n",
        "        self.yoloMLP = nn.Sequential(\n",
        "            nn.Linear(num_keypoints * 2, 128),\n",
        "            nn.LeakyReLU(lrelu_tr),\n",
        "            nn.Dropout(dropout_mlp),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.LeakyReLU(lrelu_tr),\n",
        "            nn.Dropout(dropout_mlp))\n",
        "\n",
        "        # Combined MLP with dropout after activation\n",
        "        self.combinedMLP = nn.Sequential(\n",
        "            nn.Linear(128 * 28 * 28 + 64, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_mlp),\n",
        "            nn.Linear(1024, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_mlp),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_mlp),\n",
        "            nn.Linear(128, num_classes))\n",
        "\n",
        "    def forward(self, x, keypoints):\n",
        "        # CNN\n",
        "        x = self.cnn(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        # YOLO MLP\n",
        "        keypoints = keypoints.view(keypoints.size(0), -1)\n",
        "        yoloMLP_out = self.yoloMLP(keypoints)\n",
        "        # Concatenation\n",
        "        concat_features = torch.cat((x, yoloMLP_out), dim=1)\n",
        "        # Combined MLP\n",
        "        output = self.combinedMLP(concat_features)\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "ffj7uml8a2lo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PoseNetResnet_Drop(nn.Module):\n",
        "    def __init__(self, num_keypoints=17, num_classes=1, lrelu_tr=0.1, pretrained=True, dropout_rate=0.2):\n",
        "        super(PoseNetResnet_Drop, self).__init__()\n",
        "        self.name = \"PoseNetResnet_Drop\"\n",
        "        # Load a pretrained ResNet18 model\n",
        "        self.resnet = models.resnet18(pretrained=pretrained)\n",
        "        # Remove the fully connected layer of ResNet18\n",
        "        self.resnet = nn.Sequential(*(list(self.resnet.children())[:-2]))\n",
        "        # MLP for processing keypoints, with dropout after activations\n",
        "        self.yoloMLP = nn.Sequential(\n",
        "            nn.Linear(num_keypoints * 2, 128),\n",
        "            nn.LeakyReLU(lrelu_tr),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.LeakyReLU(lrelu_tr),\n",
        "            nn.Dropout(dropout_rate))\n",
        "\n",
        "        # Combined MLP with dropout after activations\n",
        "        self.combinedMLP = nn.Sequential(\n",
        "            nn.Linear(512 * 7 * 7 + 64, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(1024, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(128, num_classes))\n",
        "\n",
        "    def forward(self, x, keypoints):\n",
        "        # Process images through the ResNet18 model\n",
        "        x = self.resnet(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        # Process keypoints through the yoloMLP\n",
        "        keypoints = keypoints.view(keypoints.size(0), -1)\n",
        "        yoloMLP_out = self.yoloMLP(keypoints)\n",
        "        # Concatenate features\n",
        "        concat_features = torch.cat((x, yoloMLP_out), dim=1)\n",
        "        # Final classification through the combined MLP\n",
        "        output = self.combinedMLP(concat_features)\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "LywVIy78bCku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  **2.3.5 Discussion on Validation Result**"
      ],
      "metadata": {
        "id": "s6iNbH6VqJEa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The validation accuracy and F1 scores for the different models are summarized as follows:"
      ],
      "metadata": {
        "id": "kwnaxF80x3-O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://i.postimg.cc/T2b568Qh/val-acc.jpg\" width=\"1000\" height=\"500\">\n",
        "\n",
        "Figure 10. Summary of Validation Results\n"
      ],
      "metadata": {
        "id": "S0Fdv3QNxkpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A notable observation from the confusion matrix is the significant reduction in false positive and false negative with the incorporation of pose information in the PoseNet+ResNet18 model. Compared to the standard ResNet model, which often misclassified individuals walking with a bicycle, the inclusion of keypoint information allows the enhanced model to accurately discern a person's posture, effectively differentiating between walking and riding.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pjA7cJQHqft7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://i.postimg.cc/RVQ3WckB/cm.jpg\"  width=\"1000\" height=\"400\">\n",
        "\n",
        "Figure 11: Comparison between ResNet with and without PoseNet"
      ],
      "metadata": {
        "id": "z_yW4Wj9y2RQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2.4 Model Testing on the Best Model**"
      ],
      "metadata": {
        "id": "AnAU2bvqpH2M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2.4.1 Model Testing**"
      ],
      "metadata": {
        "id": "9dBl185DEi1_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the results from training and validation, the ResNet18 model enhanced with pose information (ResNet18 + Pose) demonstrated the best performance. Therefore, we used the test dataset to further evaluate this model's performance on new data."
      ],
      "metadata": {
        "id": "pfS2bvxlpkD2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The get_accuracy function calculates the model's accuracy on the test dataset and provides a confusion matrix along with images that were incorrectly classified."
      ],
      "metadata": {
        "id": "TiUpkX8PuZSe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_accuracy(model, data_loader, PoseNetModel):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    correct, total = 0, 0\n",
        "    misclassified_samples = []\n",
        "    label_list, pred_list = [], []\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in data_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            res = PoseNetModel(inputs, verbose=False)\n",
        "            res_keypoints = torch.stack([i.keypoints[0].xy if i.keypoints[0].conf!=None else torch.zeros(1,17,2).to(device) for i in res ]).squeeze(1)\n",
        "\n",
        "            ################\n",
        "            mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1).to(device)\n",
        "            std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1).to(device)\n",
        "            inputs = (inputs - mean) / std\n",
        "\n",
        "            outputs = model(inputs, res_keypoints)  # Normalize inputs\n",
        "            preds = torch.round(torch.sigmoid(outputs))\n",
        "\n",
        "            correct += (preds.squeeze(1) == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "            label_list.extend(labels.cpu().numpy())\n",
        "            pred_list.extend(preds.squeeze(1).detach().cpu().numpy())\n",
        "\n",
        "            misclassified = (preds.squeeze(1) != labels)\n",
        "            for i, mis in enumerate(misclassified):\n",
        "                if mis:\n",
        "                    misclassified_samples.append((inputs[i].cpu(), labels[i].cpu(), preds[i].cpu()))\n",
        "\n",
        "        # Recalculate or retrieve predictions for the best_acc_epoch if necessary\n",
        "        cm = confusion_matrix(label_list, pred_list)\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=[0, 1], yticklabels=[0, 1])\n",
        "        plt.title(f'Confusion Matrix for Test Set')\n",
        "        plt.ylabel('True Label')\n",
        "        plt.xlabel('Predicted Label')\n",
        "        plt.show()\n",
        "\n",
        "        show_misclassified_images(misclassified_samples)\n",
        "\n",
        "    return correct / total"
      ],
      "metadata": {
        "id": "IuZ6w8ktpUc3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the test data:"
      ],
      "metadata": {
        "id": "FFGIM7xJvJHG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "id": "SB5sXneqpY_L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test the model with test dataset:"
      ],
      "metadata": {
        "id": "uf0Vw-Ms0o9Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "yoloPoseModel = YOLO('F:/UT_MIE/MIE1517/Project/yolov8n-pose.pt')\n",
        "best_model = PoseNetCNN()\n",
        "model_dir = 'models'\n",
        "best_filename = \"PoseNetCNN_best.pth\"\n",
        "best_path = os.path.join(model_dir, best_filename)\n",
        "best_model.load_state_dict(torch.load(best_path))\n",
        "\n",
        "# Test on the unseen test dataset\n",
        "test_acc = get_accuracy(best_model, test_loader, yoloPoseModel)\n",
        "print(\"The test accuracy of best model is: \", test_acc)"
      ],
      "metadata": {
        "id": "XJtjRXuJpbbu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2.4.2 Integrated system**"
      ],
      "metadata": {
        "id": "TeZGQDX7Y8F3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also developed a function that systematically processes video input, utilizing our pipeline to identify cyclists and classify their riding status. This function, along with our test results, serves as a tool for evaluating our model's performance."
      ],
      "metadata": {
        "id": "11wlRWfGfXyw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "The function cyclist_detect_system serves as our integrated system that takes in a\n",
        "raw video and performs cyclist biking status classification. The annotated frames\n",
        "and videos will be stored in a folder.\n",
        "\n",
        "Note that in the annotated videos, the bounding boxes are cyclist bounding boxes\n",
        "and the green dots are the 17 keypoints on human stem. We highlight the riding\n",
        "class with red bounding box and green bouding box for unride.\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "def resize_and_pad(img_size):\n",
        "    return transforms.Compose([\n",
        "        transforms.Resize(img_size, interpolation=transforms.InterpolationMode.BILINEAR),\n",
        "        transforms.CenterCrop(img_size),\n",
        "        transforms.ToTensor(),\n",
        "        # Optional: Apply normalization if needed\n",
        "        # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ])\n",
        "\n",
        "def pad_to_square(img, fill=0):\n",
        "    max_size = max(img.size)\n",
        "    padding = [0, 0, max_size - img.size[0], max_size - img.size[1]]  # Calculate padding\n",
        "    return transforms.Pad(padding, fill=fill)(img)\n",
        "\n",
        "def cyclist_detect_system(data_home_dir, img_out_dir, vid_in_dir, model, cls, PoseNetModel,PoseNetCNN, yolo_imgsz=640, out_fps=3,vid_out_dir=None, box_conf_tr=0, device=\"cpu\"):\n",
        "\n",
        "    ####################\n",
        "    inference_transform = transforms.Compose([\n",
        "            transforms.Lambda(lambda img: pad_to_square(img, fill=0)),\n",
        "            resize_and_pad((224, 224)),\n",
        "            # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        ])\n",
        "\n",
        "    PoseNetModel = PoseNetModel.to(device)\n",
        "    PoseNetCNN  = PoseNetCNN.to(device)\n",
        "    #####################\n",
        "\n",
        "    vid_in_name,vid_in_ext = os.path.splitext(os.path.basename(vid_in_dir))\n",
        "\n",
        "    # load video and start recieving output\n",
        "    cap = cv2.VideoCapture(vid_in_dir)\n",
        "    print(f\"processing vid: {vid_in_dir}\")\n",
        "    if not cap.isOpened():\n",
        "        print(f\"cap is not opened: status-{cap.isOpened()}\")\n",
        "        sys.exit()\n",
        "    # get specs of video\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    # print(\"here\")\n",
        "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    # extract part of the frames only to save computation and storage\n",
        "    skip_frames = int(fps / out_fps)\n",
        "    ## output to mp4\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    vid_out_dir = os.path.join(img_out_dir ,vid_in_name) # output video to same folder as frames\n",
        "    os.makedirs(vid_out_dir, exist_ok=True)\n",
        "    video_out = cv2.VideoWriter(os.path.join(vid_out_dir,f'res_{vid_in_name}{vid_in_ext}'), fourcc, out_fps, (frame_width, frame_height))\n",
        "\n",
        "    #### process each iamge with yolo\n",
        "    frame_id = 0\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret,frame = cap.read()\n",
        "        # print(\"cap ret frame read\")\n",
        "        if ret:\n",
        "            # skip frames\n",
        "            if frame_id % skip_frames == 0:\n",
        "                frame_rgb = frame # cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "                # save curr frame\n",
        "                # cv2.imwrite(f'{output_path}/{frame_id}.jpg', frame_rgb)\n",
        "\n",
        "                # inference\n",
        "                results = model([frame_rgb], imgsz=yolo_imgsz, verbose=False)\n",
        "                #\n",
        "                # out.write(results.numpy())\n",
        "                # #\n",
        "                for img_index, result in enumerate(results):\n",
        "                    frame_folder_dir  = f'{vid_in_name}/res_img_{img_index}_frame_{frame_id}'\n",
        "                    frame_output_filename = f'{cls}_res_yolo_vid_{vid_in_name}_img_{img_index}_fid_{frame_id}.jpg'\n",
        "                    frame_folder_dir = os.path.join(img_out_dir, frame_folder_dir)\n",
        "                    os.makedirs(frame_folder_dir, exist_ok=True)\n",
        "\n",
        "                    # print(img_index)\n",
        "                    # save yolo annoated result\n",
        "                    result.save(filename=f\"{frame_folder_dir}/{frame_output_filename}\")\n",
        "                    result_array = result.plot(labels=False, probs=False, boxes=False)\n",
        "                    # retireve all the bbox predicted by yolo, here, only classes labeled as human or bike are gathered\n",
        "                    # these resultes are stored in a dict which will then be further processed\n",
        "                    person_bbox_list, bike_bbox_list = [],[]\n",
        "                    bbox_cls_dict = result.names\n",
        "                    boxes_labs = result.boxes.cls.tolist()\n",
        "                    boxes_conf = result.boxes.conf.tolist()\n",
        "                    for box_id, (box, box_label, box_conf) in enumerate(zip(result.boxes.xyxy, boxes_labs,boxes_conf)):\n",
        "                        bbox_path=f\"{frame_folder_dir}/box_id_{box_id}_{bbox_cls_dict[box_label]}.jpg\"\n",
        "                        x1,y1,x2,y2 = [int(_) for _ in box.tolist()]\n",
        "                        x1, y1 = max(x1, 0), max(y1, 0)\n",
        "                        x2, y2 = min(x2, frame_width), min(y2, frame_height)\n",
        "                        # print(x1,y1,x2,y2)\n",
        "                        # cropped_img = frame_rgb[y1:y2, x1:x2]\n",
        "                        # cv2.imwrite(bbox_path, cropped_img)\n",
        "                        # store the current bounding box\n",
        "                        #################################\n",
        "                        if box_conf>box_conf_tr:\n",
        "                            pb_bbox_path=f\"{frame_folder_dir}/{cls}_res_cropbp_vid_{vid_in_name}_img_{img_index}_fid_{frame_id}_bbox_{box_id}.jpg\"\n",
        "                            pb_bbox_img = frame_rgb[y1:y2, x1:x2]\n",
        "                            cv2.imwrite(pb_bbox_path, pb_bbox_img)\n",
        "\n",
        "                            #\n",
        "                            pb_bbox_img = cv2.cvtColor(pb_bbox_img, cv2.COLOR_BGR2RGB)\n",
        "                            #\n",
        "                            device=\"cuda\"\n",
        "                            pb_bbox_img_tran = inference_transform(Image.fromarray(pb_bbox_img)).unsqueeze(0).to(device)\n",
        "                            PoseNetCNN.eval()\n",
        "                            with torch.no_grad():\n",
        "                                res = PoseNetModel(pb_bbox_path, verbose=False)\n",
        "                                if res[0].keypoints.conf!=None:\n",
        "                                    res_keypoints_np = res[0].keypoints[0].xy[0]\n",
        "                                    res_keypoints = torch.tensor(res[0].keypoints.xy[0].unsqueeze(0))   #torch.stack([i.keypoints.xy if i.keypoints.conf!=None else torch.zeros(1,17,2).to(device) for i in res ]).squeeze(1).to(device)\n",
        "\n",
        "                                    keypoints_large_img = [(x + x1, y + y1) for (x, y) in res_keypoints_np if not x==y==0.0]\n",
        "\n",
        "                                    for (x, y) in keypoints_large_img:\n",
        "                                        cv2.circle(frame_rgb, (int(x), int(y)), radius=3, color=(0, 255, 0), thickness=-1)\n",
        "                                        cv2.circle(result_array, (int(x), int(y)), radius=3, color=(0, 255, 0), thickness=-1)\n",
        "                                else:\n",
        "                                    res_keypoints = torch.zeros(1,17,2).to(device)\n",
        "                                    # print(\"reach 0\")\n",
        "\n",
        "                                ################ normalize the input image\n",
        "                                mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1).to(device)\n",
        "                                std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1).to(device)\n",
        "                                pb_bbox_img_tran = (pb_bbox_img_tran - mean) / std\n",
        "                                ################\n",
        "\n",
        "                                curr_bbox_outputs = PoseNetCNN(pb_bbox_img_tran, res_keypoints)\n",
        "                                preds = torch.round(torch.sigmoid(curr_bbox_outputs))\n",
        "\n",
        "                                #\n",
        "                                label={0:\"ride\",1:\"unride\"}[preds.item()]\n",
        "                                color_ = {0:(0, 0, 255), 1:(0, 255, 0)}[preds.item()]\n",
        "                                cv2.rectangle(result_array, (x1, y1), (x2, y2), color_, 2)\n",
        "                                cv2.putText(result_array, label, (x1, max(y1 - 10, 0)), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color_, 2)\n",
        "\n",
        "                        #################################\n",
        "\n",
        "\n",
        "\n",
        "                    video_out.write(result_array) # f'{cls}_res_img_{img_index}_fid_{frame_id}.jpg'\n",
        "                    cv2.imwrite(f\"{frame_folder_dir}/{cls}_res_yolobp_vid_{vid_in_name}_img_{img_index}_fid_{frame_id}.jpg\", result_array)\n",
        "\n",
        "            frame_id += 1\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    # print(\"here end\")\n",
        "    cap.release()\n",
        "    video_out.release()"
      ],
      "metadata": {
        "id": "-V5UAruzfQzS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This code serves as our integrated system, this takes in all the raw videos in\n",
        "# a folder and perform cyclist riding status classification. The results will be\n",
        "# stored in a seperate file containing the results for each frame and the anntated\n",
        "# video.\n",
        "\n",
        "objDetectModel = YOLO('/home/yuliang/1517proj/ultralytics/checkpoints/yolov8n_e46_v0.pt')\n",
        "PoseNetModel = YOLO('/home/yuliang/1517proj/ultralytics/checkpoints/yolov8n-pose.pt')\n",
        "PoseNetCNN_ = PoseNetCNN3() #torch.load('/home/yuliang/1517_proj/ultralytics/checkpoint/PoseNetResnet_96') #PoseNetResnet_96\n",
        "PoseNetCNN_.load_state_dict(torch.load('/home/yuliang/1517proj/ultralytics/checkpoints/PoseNetCNN3/PoseNetCNN3_best.pth'))\n",
        "# PoseNetCNN = torch.load('/home/yuliang/1517_proj/ultralytics/checkpoint/PoseNetCNN/PoseNetCNN_best.pth')\n",
        "\n",
        "#\n",
        "cls_list=['test']\n",
        "# cls=cls_list[0]\n",
        "for cls in cls_list:\n",
        "    task_name = \"bike_data\"\n",
        "    data_home_dir=\"/home/yuliang/1517proj/ultralytics/datasets/test_vid\"\n",
        "    img_out_dir=f\"/home/yuliang/1517proj/ultralytics/datasets/test_vid/v1/{task_name}_img/\"\n",
        "    vid_out_dir=f\"/home/yuliang/1517proj/ultralytics/datasets/test_vid/v1/{task_name}_vid/\"\n",
        "    os.makedirs(img_out_dir, exist_ok=True)\n",
        "    os.makedirs(vid_out_dir, exist_ok=True)\n",
        "\n",
        "    raw_data_dir = f\"/home/yuliang/1517proj/ultralytics/datasets/test_vid\"\n",
        "    for file in os.listdir(raw_data_dir):\n",
        "        if file.endswith('.mp4'):\n",
        "            vid_in_dir = os.path.join(raw_data_dir, file)\n",
        "            cyclist_detect_system(data_home_dir, img_out_dir, vid_in_dir, device=\"cuda:0\",\n",
        "                               model=objDetectModel,\n",
        "                               PoseNetModel=PoseNetModel,\n",
        "                               PoseNetCNN=PoseNetCNN_,\n",
        "                                 cls=cls, yolo_imgsz=640, out_fps=5,box_conf_tr=0.5 )"
      ],
      "metadata": {
        "id": "KDyu-2dmYxpP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r0IaaOB-YriR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2.4.3 Testing Result & Discussion**"
      ],
      "metadata": {
        "id": "tg3wpzdSEsd1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we dive into the final test results, it's important to note that our current validation accuracy and F1 score differ from what was shared during the presentation. In the presentation, we mentioned that the test accuracy we got at that time was about 65%. We did some troubleshooting and found the problem: the YOLO model has its own image transform methods, which conflicts with our image transform, leading to a failure to detect stem information. For images without keypoints information, we simply set the input parameter to 0. Besides, some images generate multiple stems because there are many people in one image or due to other problems. After presentation, We modified our image transform and the training loop to extract the stem information and fixed the bug."
      ],
      "metadata": {
        "id": "4DoxkYY-ExC0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After implementing the new image transformation method and adjusting the training loop to extract stem information from images, our test accuracy improved significantly. The PoseNet + ResNet18 model reached an 84% accuracy on the test set, which aligns with its validation accuracy. Surprisingly, the PoseNet + Simple CNN model achieved an 88.8% accuracy on the test set, not only surpassing the PoseNet model with ResNet18 but also its own validation accuracy. Furthermore, the fine-tuned PoseNet + Simple CNN model outperformed the PoseNet + ResNet18 in cyclist detection systems processing new video input.\n",
        "\n",
        "[due to data distribution, picked new videos are hard to identify]\n",
        "Besides, we observed that our model did not seems like achieved a 80% accuracy on new, unseen videos featuring a mix of riding and non-riding individuals. Below are example outputs (selected frames from the entire video) from our cyclist detection system. The first image shows perfect performance, but the second and third images exhibit some misclassifications."
      ],
      "metadata": {
        "id": "Zz7W3z8ge7aF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://i.postimg.cc/5ypVVNRZ/test-res-yolobp-vid-mixed-1-img-0-fid-0.jpg\"  width=\"500\" height=\"280\">\n",
        "\n",
        "Figure 12 (a): Good Performance\n",
        "\n",
        "<img src=\"https://i.postimg.cc/wxkHJ0j0/test-res-yolobp-vid-mixed-2-img-0-fid-130.jpg\"  width=\"500\" height=\"280\">\n",
        "\n",
        "Figure 12 (b): Misclassification\n",
        "\n",
        "<img src=\"https://i.postimg.cc/3JJhZmy7/test-res-yolobp-vid-mixed-4-img-0-fid-64.jpg\"  width=\"500\" height=\"280\">\n",
        "\n",
        "Figure 12 (c): Misclassification"
      ],
      "metadata": {
        "id": "HeEBSuTvhPnE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "We identified two main problems may cause the issues.\n",
        "\n",
        "Firstly, we found our model exhibits variable performance across different test videos, facing particular challenges in distinguishing between 'unride' and 'ride' states of cyclists from specific viewpoints. As an example, in the second and third images shown above that contain misclassifications, we find that the locations of the keypoints (green dots) are very similar for ride and unride when viewed from the front or back. This means that in some angles of the cyclists, our keypoint data only provide limited valid information.\n",
        "\n",
        "Interestingly, the model underperforms when interpreting side perspectives, which are depicted in the image below. This difficulty arises from the two-dimensional nature of the images, where it is challenging to differentiate between a cyclist stationary with one foot on the ground and a pedestrian walking alongside a bicycle. Furthermore, the dataset comprises images captured from various orientations. An imbalanced distribution of these orientations may inadvertently lead the model to favor the prediction of the most prevalent orientation rather than accurately classifying 'unride' or 'ride' states.\n"
      ],
      "metadata": {
        "id": "TXv4c4b5379J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "<img src=\"https://i.postimg.cc/Nfxw8tNx/sideview2.png\"  width=\"650\" height=\"350\">\n",
        "\n",
        "Figure 13: Misclassifcation From Side View"
      ],
      "metadata": {
        "id": "Zhs9lfa0Fwqz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Secondly, our team recognizes the need to enhance the diversity of our dataset. Due to time constraints and hardware limitations, we initially sourced data exclusively from online videos. We take different frames from multiple videos and use a bounding box to crop images containing people and bicycles. This approach, while practical, resulted in a dataset with similar images from the same videos, potentially reducing the model's ability to generalize across varied data patterns. Moreover, to expedite training, we had to limit the dataset size, further constraining our model's learning potential and leading to results that may not fully reflect its capabilities.\n",
        "\n",
        "Moving forward, we aim to improve both the quality and quantity of the data to bolster the model's training process. By expanding our dataset with more diverse sources and images, we can better train our model to recognize and differentiate between a wider range of scenarios, enhancing its reliability and performance."
      ],
      "metadata": {
        "id": "aTnC7CCIiANR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Related Work & Discussion**"
      ],
      "metadata": {
        "id": "Q_w6Rfuta6Ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3.1 Summary of Related Work**"
      ],
      "metadata": {
        "id": "LryMMYG45hVR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our project has greatly benefited from insights provided by the paper titled 'On the safety of vulnerable road users by cyclist orientation detection using Deep Learning' by Garcia-Venegas et al. (2020).\n",
        "\n",
        "The paper’s emphasis on the safeguarding of vulnerable road users resonated with our mission, deepening our understanding beyond the technical aspects to the project's societal impact.\n",
        "Garcia-Venegas and colleagues evaluated various models for detecting cyclists on roads, including SSD, Faster R-CNN, and R-FCN, in conjunction with ResNet50 and ResNet101 architectures. While ResNet50 outperformed others in precision, its slower processing rate was notable. Alternatively, SSD paired with InceptionV2 struck a balance between precision and speed [3]. Speed of detection being crucial for real-time application, based on this paper's findings, our project has adopted YOLO V8 (You Only Look Once) as the foundational model to detect both stationary and moving cyclists."
      ],
      "metadata": {
        "id": "s0vGUSX7U4A9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3.2 Discussion**"
      ],
      "metadata": {
        "id": "KwTEphkzvc7y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3.2.1 Findings & Future Work**"
      ],
      "metadata": {
        "id": "jpZCM9LAvowf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://i.postimg.cc/Dwy3BNH3/8-angels.png\"  width=\"400\" height=\"400\">\n",
        "\n",
        "Figure 14: Proposed Eight Orientations [3, Fig4]"
      ],
      "metadata": {
        "id": "3vATqt-SXlSO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To address the issues we identified earlier, we have come up with several ideas.\n",
        "\n",
        "In future, a strategy for model refinement could involve adopting the approach from the paper 'On the safety of vulnerable road users by cyclist orientation detection using Deep Learning.' The systematic categorization of our 'unride/ride' dataset into distinct orientation-based groups, as suggested by Figure 14, may generate improvements in our model’s capability to distinguish 'unride' and 'ride' states from varied angles. Implementing this strategy could mitigate the challenges caused by orientation disparities while concurrently supplying our detection system with detailed orientation data. This enhancement has the potential to raise the model's accuracy and reliability. However, assimilating this additional information necessitates expanding the model's complexity, which will likely involve integrating more layers and parameters. Such expansion is critical to ensure that our model can process and learn from the richer, more complex dataset effectively.\n",
        "\n",
        "Alongside these, we can also leverage information beyond the keypoints to gain additional insights into the rider's posture. For instance, calculating the angle and length between different parts of the stem or limbs could be beneficial. Such information, like the angle between the thigh and calf, may help us predict riding behavior more accurately.\n",
        "\n",
        "In addition, we can also continue to look for high-quality training, validation, and test images to increase the diversity of the dataset. The use of online videos for testing, while practical, is not without its limitations, particularly in representing the diversity of real-world conditions. The severe weather that impeded our ability to shoot original footage meant that we could not capture the nuanced variables that occur in natural settings. In future endeavors, if we have the opportunity to test our model with data derived from actual street scenarios, the reliability of our test scores is poised to improve. Real street data would introduce a spectrum of environmental factors—such as varying weather conditions, lighting, and traffic scenarios—that are crucial for a comprehensive assessment of the model's performance and its applicability in real-world situations. On top of that, we can also appropriately apply game screenshots, modeling images, AI-generated images, etc. to enrich our data sources. These avenues can break through our data collection limitations to a certain extent."
      ],
      "metadata": {
        "id": "uzLvpNGoPmTm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3.2.2 Lessons Learned**"
      ],
      "metadata": {
        "id": "N5Oyt3db4oQH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Throughout this practical project, our group has gained a deeper appreciation for the significance of dataset quality and the necessity of diligent monitoring during the training process. In many instances, suitable datasets are not readily available; our project was no exception. While there was an abundance of data for cyclists in motion (ride state), we faced a scarcity when it came to the stationary (unride state) images, prompting us to source data from online videos and alternative outlets.\n",
        "\n",
        "The confusion matrix became a pivotal tool for us, revealing misclassified samples and offering insights into the training process, which allowed us to identify and address issues within our dataset. For instance, the initial iterations of our customized YOLO model would occasionally detect only a portion of a cyclist's body. These errors were brought to our attention during training, as depicted in the image below. Through refinement of the customized YOLO model, we were able to enhance the quality of our dataset. This improvement underscored a fundamental lesson: the quality of the dataset is paramount for optimal model performance."
      ],
      "metadata": {
        "id": "gPwEVWCGFm5a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://i.postimg.cc/mg6JVg77/image-improve.png\"  width=\"800\" height=\"400\">\n",
        "\n",
        "Figure 15: Improved Yolo Image Quality"
      ],
      "metadata": {
        "id": "nEjG97wZX82y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. Final words**"
      ],
      "metadata": {
        "id": "OlKkP8MNDNCD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Our group really enjoyed this project and learned a lot from it. First, we want to say a big thank you to Professor Sinisa Colic and our teaching assistants. They gave us a lot of good advice and showed us how to improve our model, like adding pose information and the best ways to train it. They also gave us flowcharts which made things clearer and helped us stay on the right path.\n",
        "\n",
        "Getting the chance to work on a project from start to finish was a great experience. Even though we faced many challenges and difficulties, we learned a lot about applying what we know about deep learning in real situations. We're also really thankful for the deep learning community online. We found a lot of projects on GitHub that inspired us and helped us with our project."
      ],
      "metadata": {
        "id": "OolP7atp3rxg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "<img src=\"https://i.postimg.cc/QM9PzZqw/4ebf8e03ad65ef21f429602203aed3f.jpg\"  width=\"600\" height=\"400\">\n",
        "\n",
        "Figure 16: Yuliang's Discussion with Professor on How to Add Pose Information."
      ],
      "metadata": {
        "id": "UeGNOOqBYHHB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5. Citation**"
      ],
      "metadata": {
        "id": "5kvVVByIEB10"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[1] C. Y. Tang, “cyclistdetectiondataset · GITLAB - dataset,” GitLab, https://gitlab03.wpi.edu/ctang5/cyclistdetectiondataset/-/tree/main (accessed Apr. 6, 2024).\n",
        "\n",
        "[2] G. Jocher, A. Chaurasia, and J. Qiu, \"Ultralytics YOLO,\" version 8.0.0, Jan. 10, 2023. [Online]. Available: https://github.com/ultralytics/yolov5. (Accessed: Apr. 6, 2024).\n",
        "\n",
        "[3] M. García-Venegas, D. A. Mercado-Ravell, L. A. Pinedo-Sánchez, and C. A. Carballo-Monsivais, “On the safety of vulnerable road users by cyclist detection and tracking,” Machine Vision and Applications, vol. 32, no. 5, Aug. 2021. doi:10.1007/s00138-021-01231-4 (Accessed: Apr. 6, 2024)."
      ],
      "metadata": {
        "id": "RJDKhLLMELyL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "jupyter nbconvert --to html /content/MIE1517_Final_Report.ipynb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WQ2UaFTWScIc",
        "outputId": "67e5c047-7047-4a06-e110-1378b4486a30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NbConvertApp] Converting notebook /content/MIE1517_Final_Report.ipynb to html\n",
            "[NbConvertApp] Writing 928197 bytes to /content/MIE1517_Final_Report.html\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    }
  ]
}